{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment #5: A sentence embedder\n",
    "Author: Pierre Nugues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you will implement a sentence embedder simplified from Reimers and Gurevych's Sentence-BERT: https://arxiv.org/pdf/1908.10084. S-BERT is written in PyTorch and its code is available from GitHub: https://github.com/UKPLab/sentence-transformers\n",
    "\n",
    "The objectives of the assignment are to:\n",
    "* Write a program to embed sentences\n",
    "* Use neural networks with PyTorch\n",
    "* Write a short report of 2 to 3 pages to describe your program.\n",
    "\n",
    "Note: Should your machine be unable to train a model for the whole dataset, then use only a fraction of the dataset such as 10% or less. For this, use the `MINI_CORPUS` constant. See below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "We saw we can vectorize words using a dense representation. We can extend this to documents. This enables us to store the resulting vectors in databases and then use fast algorithms for paragraph or document comparisons such as Faiss: https://github.com/facebookresearch/faiss\n",
    "\n",
    "There are many document vectorization techniques and models are regularly benchmarked, see: https://huggingface.co/spaces/mteb/leaderboard. See also a list of available vector databases here https://db-engines.com/en/ranking/vector+dbms\n",
    "\n",
    "In this lab, you will program two techniques to vectorize documents into dense vectors. You will first implement a baseline technique and then a toy version of SBERT. SBERT is one of the earliest transformer-based document vectorization algorithm: _Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks_ by Reimers and Gurevych (2019) https://arxiv.org/pdf/1908.10084"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of the algorithms\n",
    "Read the Getting Started paragraph of https://github.com/UKPLab/sentence-transformers for an overview.\n",
    "\n",
    "Read the summary of the SBERT paper as well as Sections 1 and 3, _Introduction_ and _Model_. In the triplet objective function, an anchor is a start sample, the positive sample is close to the anchor, while the negative one is different. Considering a language detector, think of a sentence in Swedish as the anchor. A positive sample would be another sentence in Swedish and a negative one could be a sentence in English.\n",
    "\n",
    "In the _Method and program struture_ section of your report, you will summarize these sections in 10 to 15 lines. Note that a three-way softmax classifier is simply a logistic regression with three classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import regex as re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a reduced dataset for the development of your program with `MINI_CORPUS` set to `True`. Once your program is ready, you can train your model on the whole dataset (if you have the time). Set `MINI_CORPUS` to `False` then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MINI_CORPUS = True  # Set the value to True when you develop the program\n",
    "MINI_PERCENTAGE = 0.01  # Percentage of the original dataset.\n",
    "# Depending on your machine, you may even use less than 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets: SNLI\n",
    "As dataset, you will use SNLI. SNLI consists of over 500,000 lines with the text of the pairs and their labels. The authors created the dataset by giving volunteers a sentence (the premise) and asking them to write a second sentence (the hypothesis) that is either definitely true\n",
    "(entailment), that might be true (neutral), or that is definitely false (contradiction).\n",
    "\n",
    "Read the dataset description from this URL https://nlp.stanford.edu/projects/snli/ and download it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please adjust the path to fit your machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('snli_1.0/snli_1.0/snli_1.0_train.jsonl', 'r') as f:\n",
    "    dataset_list = list(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_snli = []\n",
    "for json_str in dataset_list:\n",
    "    dataset_snli += [json.loads(json_str)]\n",
    "    # print(f\"result: {result}\")\n",
    "    # print(isinstance(result, dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sample with an agreement in the annotation. The final annotation is the gold label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'annotator_labels': ['entailment'],\n",
       " 'captionID': '3706019259.jpg#3',\n",
       " 'gold_label': 'entailment',\n",
       " 'pairID': '3706019259.jpg#3r2e',\n",
       " 'sentence1': 'A foreign family is walking along a dirt path next to the water.',\n",
       " 'sentence1_binary_parse': '( ( A ( foreign family ) ) ( ( is ( ( walking ( along ( a ( dirt path ) ) ) ) ( next ( to ( the water ) ) ) ) ) . ) )',\n",
       " 'sentence1_parse': '(ROOT (S (NP (DT A) (JJ foreign) (NN family)) (VP (VBZ is) (VP (VBG walking) (PP (IN along) (NP (DT a) (NN dirt) (NN path))) (ADVP (JJ next) (PP (TO to) (NP (DT the) (NN water)))))) (. .)))',\n",
       " 'sentence2': 'A family of foreigners walks by the water.',\n",
       " 'sentence2_binary_parse': '( ( ( A family ) ( of foreigners ) ) ( ( walks ( by ( the water ) ) ) . ) )',\n",
       " 'sentence2_parse': '(ROOT (S (NP (NP (DT A) (NN family)) (PP (IN of) (NP (NNS foreigners)))) (VP (VBZ walks) (PP (IN by) (NP (DT the) (NN water)))) (. .)))'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_snli[300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sample with no agreement in the annotation. The gold label is `_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'annotator_labels': ['contradiction', 'contradiction', 'neutral', 'neutral'],\n",
       " 'captionID': '2677109430.jpg#2',\n",
       " 'gold_label': '-',\n",
       " 'pairID': '2677109430.jpg#2r1c',\n",
       " 'sentence1': 'A small group of church-goers watch a choir practice.',\n",
       " 'sentence1_binary_parse': '( ( ( A ( small group ) ) ( of church-goers ) ) ( ( watch ( a ( choir practice ) ) ) . ) )',\n",
       " 'sentence1_parse': '(ROOT (S (NP (NP (DT A) (JJ small) (NN group)) (PP (IN of) (NP (NNS church-goers)))) (VP (VBP watch) (NP (DT a) (NN choir) (NN practice))) (. .)))',\n",
       " 'sentence2': 'A choir performs in front of packed crowd.',\n",
       " 'sentence2_binary_parse': '( ( A choir ) ( ( performs ( in ( front ( of ( packed crowd ) ) ) ) ) . ) )',\n",
       " 'sentence2_parse': '(ROOT (S (NP (DT A) (NN choir)) (VP (VBZ performs) (PP (IN in) (NP (NP (NN front)) (PP (IN of) (NP (JJ packed) (NN crowd)))))) (. .)))'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_snli[145]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove all the samples that have no agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_str = []\n",
    "for sample in dataset_snli:\n",
    "    s1 = sample['sentence1']\n",
    "    s2 = sample['sentence2']\n",
    "    label = sample['gold_label']\n",
    "    if label != '-':\n",
    "        dataset_str += [(s1, s2, label)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "549367"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('A person on a horse jumps over a broken down airplane.',\n",
       " 'A person is training his horse for a competition.',\n",
       " 'neutral')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('A person on a horse jumps over a broken down airplane.',\n",
       " 'A person is at a diner, ordering an omelette.',\n",
       " 'contradiction')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_str[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('A person on a horse jumps over a broken down airplane.',\n",
       " 'A person is outdoors, on a horse.',\n",
       " 'entailment')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_str[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MINI_CORPUS:\n",
    "    new_size = int(len(dataset_str) * MINI_PERCENTAGE)\n",
    "    dataset_str = dataset_str[:new_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5493"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets: GloVe\n",
    "\n",
    "You will first implement a baseline, an easy technique that serves as comparison for more elaborate ones. \n",
    "\n",
    "In Sect. 4.1 and Table 1 the authors proposed a baseline technique for computing a semantic\n",
    "textual similarity between two sentences that uses GloVe embeddings. Describe this technique in 5 to 10 lines in the _Method and program struture_ section. You will create a *Baseline* subsection for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe\n",
    "You will use a list of pretrained word embeddings to implement the baseline and GloVe is one such vector lists. GloVe is available in different dimensionalities (50, 100, 200, 300) and vocabulary sizes (400,000 words, 1.2M, 2.4M). \n",
    "\n",
    "Download the GloVe 6B embeddings from https://nlp.stanford.edu/projects/glove/, uncompress it, and keep the `glove.6B.50d.txt` file of 400,000 words with 50-dimensional vectors.\n",
    "\n",
    "Please adjust your path to read the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_file = 'GloVe data/glove.6B.50d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_embeddings(file):\n",
    "    \"\"\"\n",
    "    Return the embeddings in the from of a dictionary\n",
    "    :param file:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    embeddings = {}\n",
    "    glove = open(file, encoding='utf8')\n",
    "    for line in glove:\n",
    "        values = line.strip().split()\n",
    "        word = values[0]\n",
    "        vector = torch.tensor(\n",
    "            list(map(float, values[1:])), dtype=torch.float32)\n",
    "        embeddings[word] = vector\n",
    "    glove.close()\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = read_embeddings(embedding_file)\n",
    "embedded_words = sorted(list(embeddings.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4.1800e-01,  2.4968e-01, -4.1242e-01,  1.2170e-01,  3.4527e-01,\n",
       "        -4.4457e-02, -4.9688e-01, -1.7862e-01, -6.6023e-04, -6.5660e-01,\n",
       "         2.7843e-01, -1.4767e-01, -5.5677e-01,  1.4658e-01, -9.5095e-03,\n",
       "         1.1658e-02,  1.0204e-01, -1.2792e-01, -8.4430e-01, -1.2181e-01,\n",
       "        -1.6801e-02, -3.3279e-01, -1.5520e-01, -2.3131e-01, -1.9181e-01,\n",
       "        -1.8823e+00, -7.6746e-01,  9.9051e-02, -4.2125e-01, -1.9526e-01,\n",
       "         4.0071e+00, -1.8594e-01, -5.2287e-01, -3.1681e-01,  5.9213e-04,\n",
       "         7.4449e-03,  1.7778e-01, -1.5897e-01,  1.2041e-02, -5.4223e-02,\n",
       "        -2.9871e-01, -1.5749e-01, -3.4758e-01, -4.5637e-02, -4.4251e-01,\n",
       "         1.8785e-01,  2.7849e-03, -1.8411e-01, -1.1514e-01, -7.8581e-01])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model = next(iter(embeddings.values())).size()[0]\n",
    "d_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read all the words in GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_words = []\n",
    "glove = []\n",
    "for word, vector in embeddings.items():\n",
    "    glove_words += [word]\n",
    "    glove += [vector]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', ',', '.', 'of', 'to', 'and', 'in', 'a', '\"', \"'s\"]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we create a tensor with the vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = torch.stack(glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4.1800e-01,  2.4968e-01, -4.1242e-01,  1.2170e-01,  3.4527e-01,\n",
       "        -4.4457e-02, -4.9688e-01, -1.7862e-01, -6.6023e-04, -6.5660e-01,\n",
       "         2.7843e-01, -1.4767e-01, -5.5677e-01,  1.4658e-01, -9.5095e-03,\n",
       "         1.1658e-02,  1.0204e-01, -1.2792e-01, -8.4430e-01, -1.2181e-01,\n",
       "        -1.6801e-02, -3.3279e-01, -1.5520e-01, -2.3131e-01, -1.9181e-01,\n",
       "        -1.8823e+00, -7.6746e-01,  9.9051e-02, -4.2125e-01, -1.9526e-01,\n",
       "         4.0071e+00, -1.8594e-01, -5.2287e-01, -3.1681e-01,  5.9213e-04,\n",
       "         7.4449e-03,  1.7778e-01, -1.5897e-01,  1.2041e-02, -5.4223e-02,\n",
       "        -2.9871e-01, -1.5749e-01, -3.4758e-01, -4.5637e-02, -4.4251e-01,\n",
       "         1.8785e-01,  2.7849e-03, -1.8411e-01, -1.1514e-01, -7.8581e-01])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove[0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of word embeddings (400,000) and their dimension (50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([400000, 50])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model = glove.size()[1]\n",
    "d_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reserve three special symbols: padding, unknown, and the first classification token of BERT. See the lecture on transformers for a clarification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "specials = ['[PAD]', '[UNK]', '[CLS]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[PAD]', '[UNK]', '[CLS]', 'the', ',', '.', 'of', 'to', 'and', 'in']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_words = specials + glove_words\n",
    "glove_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add the vectors for the special tokens to our tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = torch.vstack((torch.zeros((3, d_model)), glove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([400003, 50])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0.])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4.1800e-01,  2.4968e-01, -4.1242e-01,  1.2170e-01,  3.4527e-01,\n",
       "        -4.4457e-02, -4.9688e-01, -1.7862e-01, -6.6023e-04, -6.5660e-01,\n",
       "         2.7843e-01, -1.4767e-01, -5.5677e-01,  1.4658e-01, -9.5095e-03,\n",
       "         1.1658e-02,  1.0204e-01, -1.2792e-01, -8.4430e-01, -1.2181e-01,\n",
       "        -1.6801e-02, -3.3279e-01, -1.5520e-01, -2.3131e-01, -1.9181e-01,\n",
       "        -1.8823e+00, -7.6746e-01,  9.9051e-02, -4.2125e-01, -1.9526e-01,\n",
       "         4.0071e+00, -1.8594e-01, -5.2287e-01, -3.1681e-01,  5.9213e-04,\n",
       "         7.4449e-03,  1.7778e-01, -1.5897e-01,  1.2041e-02, -5.4223e-02,\n",
       "        -2.9871e-01, -1.5749e-01, -3.4758e-01, -4.5637e-02, -4.4251e-01,\n",
       "         1.8785e-01,  2.7849e-03, -1.8411e-01, -1.1514e-01, -7.8581e-01])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove[3, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "### Tokenization\n",
    "You will now tokenize the sentences\n",
    "\n",
    "Write a regular expression that tokenizes the words, numbers, and punctuation. Use Unicode classes. Note that a punctuation is a single symbol while the words and numbers are sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "pattern = '\\p{L}+|\\p{N}+|\\p{Punct}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a tokenization function that takes a string as input and results a list of tokens. Set the string in lower case by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "def tokenize(sentence, pattern, lc=True):\n",
    "    if lc:\n",
    "        sentence = sentence.lower()\n",
    "    tokens = re.findall(pattern, sentence)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'person',\n",
       " 'on',\n",
       " 'a',\n",
       " 'horse',\n",
       " 'jumps',\n",
       " 'over',\n",
       " 'a',\n",
       " 'broken',\n",
       " 'down',\n",
       " 'airplane',\n",
       " '.']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(dataset_str[0][0], pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a code that, for each sample of your dataset, builds a triple consisting of:\n",
    "1. The first tokenized sentence, \n",
    "2. The second one, and \n",
    "3. The class\n",
    "\n",
    "Build a list of all these triples to represent your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "dataset_tokens = []\n",
    "for s1, s2, label in dataset_str:\n",
    "    tokens1 = tokenize(s1, pattern)\n",
    "    tokens2 = tokenize(s2, pattern)\n",
    "    dataset_tokens += [(tokens1, tokens2, label)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['a',\n",
       "  'person',\n",
       "  'on',\n",
       "  'a',\n",
       "  'horse',\n",
       "  'jumps',\n",
       "  'over',\n",
       "  'a',\n",
       "  'broken',\n",
       "  'down',\n",
       "  'airplane',\n",
       "  '.'],\n",
       " ['a',\n",
       "  'person',\n",
       "  'is',\n",
       "  'training',\n",
       "  'his',\n",
       "  'horse',\n",
       "  'for',\n",
       "  'a',\n",
       "  'competition',\n",
       "  '.'],\n",
       " 'neutral')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5493"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build token-to-index `token2idx` and index-to-token `idx2token` dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "token2idx = {word: i for i, word in enumerate(glove_words)}\n",
    "idx2token = {i: word for i, word in enumerate(glove_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 10)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2idx['the'], token2idx['a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('the', 'a')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2token[3], idx2token[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect the set of all the labels (classes) from your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "\"\"\"\n",
    "labels_set = set()\n",
    "for s1, s2, label in dataset_tokens:\n",
    "    labels_set.add(label)\n",
    "labels = list(labels_set)\n",
    "\"\"\"\n",
    "\n",
    "labels = ['entailment', 'contradiction', 'neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['entailment', 'contradiction', 'neutral']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build label-to-index `label2idx` and index-to-label `idx2label` dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "label2idx = {label: i for i, label in enumerate(labels)}\n",
    "idx2label = {i: label for i, label in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entailment': 0, 'contradiction': 1, 'neutral': 2}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'entailment', 1: 'contradiction', 2: 'neutral'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function to convert:\n",
    "  * A list of tokens into a list of `LongTensor` indices and \n",
    "  * The class to a tensor. \n",
    "\n",
    "Your function should be able to handle two types: either a list or a string. The tokens are strored in a list and the class (label) is a string\n",
    "\n",
    "Note that an unknown token in GloVe should be mapped to the `UNK` symbol of index 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "def convert_symbols(symbols, symbol2idx):\n",
    "    if type(symbols) is str:\n",
    "        try:\n",
    "            idx = symbol2idx[symbols]\n",
    "        except KeyError:\n",
    "            idx = symbol2idx['[UNK]']\n",
    "        return torch.tensor(idx, dtype=torch.long)\n",
    "    else:\n",
    "        indices = []\n",
    "        for symbol in symbols:\n",
    "            try:\n",
    "                idx = symbol2idx[symbol]\n",
    "            except KeyError:\n",
    "                idx = symbol2idx['[UNK]']\n",
    "            indices.append(idx)\n",
    "        return torch.tensor(indices, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['a',\n",
       "  'person',\n",
       "  'on',\n",
       "  'a',\n",
       "  'horse',\n",
       "  'jumps',\n",
       "  'over',\n",
       "  'a',\n",
       "  'broken',\n",
       "  'down',\n",
       "  'airplane',\n",
       "  '.'],\n",
       " ['a',\n",
       "  'person',\n",
       "  'is',\n",
       "  'training',\n",
       "  'his',\n",
       "  'horse',\n",
       "  'for',\n",
       "  'a',\n",
       "  'competition',\n",
       "  '.'],\n",
       " 'neutral')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_tokens[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert a list of tokens into a `LongTensor` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   10,   902,    16,    10,  2870, 11073,    77,    10,  2324,   138,\n",
       "         7353,     5])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_symbols(dataset_tokens[0][0], token2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  10,  902,   17,  791,   29, 2870,   13,   10,  994,    5])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_symbols(dataset_tokens[0][1], token2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neutral'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_tokens[0][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert a label string into a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_symbols(dataset_tokens[0][2], label2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unknown tokens have the index 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'person', 'on', 'a', 'horsewww']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize('a person on a horsewww', pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 10, 902,  16,  10,   1,   1])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_symbols(tokenize('a person on a horsewww wxwx', pattern), token2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the tokens and labels in your dataset by their indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "dataset = []\n",
    "for s1, s2, label in dataset_tokens:\n",
    "    x1 = convert_symbols(s1, token2idx)\n",
    "    x2 = convert_symbols(s2, token2idx)\n",
    "    y = convert_symbols(label, label2idx)\n",
    "    dataset += [(x1, x2, y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([   10,   902,    16,    10,  2870, 11073,    77,    10,  2324,   138,\n",
       "          7353,     5]),\n",
       " tensor([    10,    902,     17,     25,     10,  19304,      4,   7490,     32,\n",
       "         119031,      5]),\n",
       " tensor(1))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch `Embedding` class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now store the GloVe vectors in a PyTorch `Embedding` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embs = nn.Embedding(glove.size()[0],\n",
    "glove.size()[1],\n",
    "padding_idx=0).from_pretrained(glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We access the embedding for _the_ with its index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.1800e-01,  2.4968e-01, -4.1242e-01,  1.2170e-01,  3.4527e-01,\n",
       "         -4.4457e-02, -4.9688e-01, -1.7862e-01, -6.6023e-04, -6.5660e-01,\n",
       "          2.7843e-01, -1.4767e-01, -5.5677e-01,  1.4658e-01, -9.5095e-03,\n",
       "          1.1658e-02,  1.0204e-01, -1.2792e-01, -8.4430e-01, -1.2181e-01,\n",
       "         -1.6801e-02, -3.3279e-01, -1.5520e-01, -2.3131e-01, -1.9181e-01,\n",
       "         -1.8823e+00, -7.6746e-01,  9.9051e-02, -4.2125e-01, -1.9526e-01,\n",
       "          4.0071e+00, -1.8594e-01, -5.2287e-01, -3.1681e-01,  5.9213e-04,\n",
       "          7.4449e-03,  1.7778e-01, -1.5897e-01,  1.2041e-02, -5.4223e-02,\n",
       "         -2.9871e-01, -1.5749e-01, -3.4758e-01, -4.5637e-02, -4.4251e-01,\n",
       "          1.8785e-01,  2.7849e-03, -1.8411e-01, -1.1514e-01, -7.8581e-01]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_embs(torch.LongTensor([3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: Mean of GloVe Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('A person on a horse jumps over a broken down airplane.',\n",
       " 'A person is training his horse for a competition.',\n",
       " 'neutral')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1_str = dataset_str[0][0]\n",
    "s2_str = dataset_str[0][1]\n",
    "s3_str = dataset_str[0][2]\n",
    "s1_str, s2_str, s3_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([   10,   902,    16,    10,  2870, 11073,    77,    10,  2324,   138,\n",
       "          7353,     5]),\n",
       " tensor([  10,  902,   17,  791,   29, 2870,   13,   10,  994,    5]),\n",
       " tensor(2))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1_idx = dataset[0][0]\n",
    "s2_idx = dataset[0][1]\n",
    "s3_idx = dataset[0][2]\n",
    "s1_idx, s2_idx, s3_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function that takes a list of indices and the GloVe embeddings as input and that computes the mean of the word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "def mean_embs(input_idx: torch.LongTensor, glove_embs: nn.Embedding) -> torch.tensor:\n",
    "    embs = glove_embs(input_idx)\n",
    "    return embs.mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([   10,   902,    16,    10,  2870, 11073,    77,    10,  2324,   138,\n",
       "          7353,     5]),\n",
       " tensor([  10,  902,   17,  791,   29, 2870,   13,   10,  994,    5]),\n",
       " tensor(2))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1988,  0.1043,  0.0180, -0.0851,  0.5251,  0.4551, -0.4729, -0.0604,\n",
       "         0.1335, -0.1824, -0.1023, -0.2145, -0.3953,  0.3100,  0.3126, -0.1235,\n",
       "        -0.1631,  0.1271, -0.8334, -0.5111,  0.0911,  0.1766, -0.1190, -0.1795,\n",
       "         0.2117, -1.6935, -0.1754,  0.3900,  0.4590, -0.1137,  3.0905, -0.0358,\n",
       "        -0.2404,  0.2918,  0.1015, -0.0099,  0.2168,  0.1239,  0.1565, -0.2061,\n",
       "        -0.1449,  0.0871, -0.1085,  0.1992, -0.0306, -0.2125,  0.1155, -0.3489,\n",
       "         0.2139, -0.1993])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_embs(dataset[0][0], glove_embs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function to compute the cosine of two vectors. You will return `torch.tensor(0.0)` if one of the vectors is zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "def compute_cosine(v1: torch.tensor, v2: torch.tensor) -> torch.tensor:\n",
    "    if torch.equal(v1, torch.zeros_like(v1)) or torch.equal(v2, torch.zeros_like(v2)):\n",
    "        return torch.tensor(0.0)\n",
    "    return torch.dot(v1, v2) / (torch.norm(v1) * torch.norm(v2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.1988,  0.1043,  0.0180, -0.0851,  0.5251,  0.4551, -0.4729, -0.0604,\n",
       "          0.1335, -0.1824, -0.1023, -0.2145, -0.3953,  0.3100,  0.3126, -0.1235,\n",
       "         -0.1631,  0.1271, -0.8334, -0.5111,  0.0911,  0.1766, -0.1190, -0.1795,\n",
       "          0.2117, -1.6935, -0.1754,  0.3900,  0.4590, -0.1137,  3.0905, -0.0358,\n",
       "         -0.2404,  0.2918,  0.1015, -0.0099,  0.2168,  0.1239,  0.1565, -0.2061,\n",
       "         -0.1449,  0.0871, -0.1085,  0.1992, -0.0306, -0.2125,  0.1155, -0.3489,\n",
       "          0.2139, -0.1993]),\n",
       " tensor([ 1.0878e-01,  3.7231e-01, -4.7114e-01, -1.3591e-02,  5.1340e-01,\n",
       "          3.7193e-01, -4.4592e-01, -5.9900e-02,  2.5352e-01, -1.3076e-01,\n",
       "          1.6231e-01,  2.3146e-03, -3.6474e-01,  2.3840e-03,  3.4653e-01,\n",
       "         -2.1769e-01,  2.5946e-02,  3.9537e-01, -6.9517e-01, -3.4811e-01,\n",
       "         -4.9454e-02,  1.4977e-01, -1.2447e-01,  8.1851e-02,  6.2581e-02,\n",
       "         -1.8692e+00, -3.1502e-01, -7.4079e-02,  1.2478e-01, -1.6717e-02,\n",
       "          3.3707e+00,  8.7725e-02, -4.0180e-01, -1.8131e-01,  2.5315e-01,\n",
       "          1.7589e-01,  2.2877e-01,  4.3286e-01, -1.6315e-02, -3.6988e-01,\n",
       "         -2.8208e-02,  3.1538e-02, -2.4721e-01,  2.5963e-01,  1.3792e-02,\n",
       "         -2.6431e-01,  1.7081e-02, -2.0042e-01,  1.6257e-01,  2.1471e-01]))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 = mean_embs(s1_idx, glove_embs)\n",
    "v2 = mean_embs(s2_idx, glove_embs)\n",
    "v1, v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50]), torch.Size([50]))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1.size(), v2.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9426)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_cosine(v1, v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now compute the cosine of pairs for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5493/5493 [00:00<00:00, 8423.01it/s]\n"
     ]
    }
   ],
   "source": [
    "cos_sim = {'entailment': 0.0, 'neutral': 0.0, 'contradiction': 0.0}\n",
    "cnt = {'entailment': 0.0, 'neutral': 0.0, 'contradiction': 0.0}\n",
    "for data in tqdm(dataset):\n",
    "    cos_val = compute_cosine(\n",
    "        mean_embs(data[0], glove_embs),\n",
    "        mean_embs(data[1], glove_embs))\n",
    "    class_name = idx2label[data[2].item()]\n",
    "    cos_sim[class_name] += cos_val\n",
    "    cnt[class_name] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will comment these values in your report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entailment': tensor(0.9453),\n",
       " 'neutral': tensor(0.9385),\n",
       " 'contradiction': tensor(0.9298)}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for key in cos_sim.keys():\n",
    "    cos_sim[key] /= cnt[key]\n",
    "cos_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SBERT: The Stacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now create the SBERT architecture and replicate the pipeline in Fig. 1 in the paper. In the next cells, we walk through the figure from the bottom to the top."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first layer builds an input consisting of two sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([   3,  360, 5453]), tensor([   3,  194,  368, 2929]))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1 = torch.LongTensor(\n",
    "    list(map(lambda x: token2idx.get(x, 1), tokenize('the small cat', pattern))))\n",
    "p2 = torch.LongTensor(\n",
    "    list(map(lambda x: token2idx.get(x, 1), tokenize('the very big dog', pattern))))\n",
    "p1, p2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we have the BERT layer. Using PyTorch classes, create an encoder of four layers where each layer has five heads. You will use the classes `TransformerEncoderLayer` and `TransformerEncoder`. The dimensionality `d_model` is 50 as this is the size of GloVe vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oscar/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "# Write your code\n",
    "encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=5)\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerEncoder(\n",
       "  (layers): ModuleList(\n",
       "    (0-3): 4 x TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=50, out_features=50, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=50, out_features=2048, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=50, bias=True)\n",
       "      (norm1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now a BERT encoder. We associate each input index to an embedding. In the next cell, we create three embedding vectors correponding to three words.\n",
    "\n",
    "In the rest of the program, all our batches will have only one sample to eliminate the need for padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7.9869e-01, 4.0992e-01, 3.9706e-01, 4.0608e-01, 5.2162e-01, 4.7479e-01,\n",
       "         7.9576e-01, 2.2609e-01, 7.4674e-01, 9.7902e-01, 5.9220e-01, 8.4274e-01,\n",
       "         6.4434e-02, 9.3559e-01, 4.8945e-01, 4.3761e-01, 3.1667e-01, 4.9250e-01,\n",
       "         3.8621e-01, 5.4175e-01, 3.3998e-01, 7.6968e-02, 8.6747e-01, 3.1925e-01,\n",
       "         6.3027e-01, 6.3631e-01, 3.9579e-01, 2.1454e-02, 6.6536e-01, 3.8259e-01,\n",
       "         1.3069e-01, 1.0481e-01, 3.7356e-01, 7.9920e-01, 2.7030e-01, 2.6574e-01,\n",
       "         5.9608e-01, 4.2225e-01, 3.8137e-01, 3.0961e-01, 9.1479e-01, 1.2154e-01,\n",
       "         8.1085e-01, 2.0580e-01, 8.6229e-01, 6.9402e-01, 1.2196e-01, 1.4650e-01,\n",
       "         6.5053e-01, 1.1256e-01],\n",
       "        [6.0683e-01, 1.7352e-01, 9.9250e-01, 4.5312e-01, 9.9307e-01, 7.3762e-01,\n",
       "         8.1816e-03, 1.5181e-01, 4.0526e-01, 1.0233e-01, 5.0846e-01, 7.0422e-01,\n",
       "         5.0080e-01, 3.1942e-01, 4.0842e-01, 4.9172e-01, 3.3720e-01, 7.1800e-01,\n",
       "         4.0909e-01, 2.7407e-02, 5.8310e-02, 9.9954e-01, 7.4428e-01, 5.2807e-01,\n",
       "         8.1909e-01, 9.4571e-01, 8.5129e-01, 2.9931e-01, 1.4329e-01, 7.4837e-01,\n",
       "         9.0513e-01, 4.9972e-01, 6.0641e-01, 5.1621e-02, 6.4568e-01, 2.4569e-01,\n",
       "         2.4667e-01, 9.7260e-01, 6.7748e-01, 7.8044e-01, 5.0830e-02, 2.5439e-04,\n",
       "         9.6292e-02, 7.3968e-02, 4.9824e-01, 9.0665e-01, 8.9005e-01, 9.2609e-02,\n",
       "         7.1507e-01, 6.4059e-01],\n",
       "        [2.3687e-01, 8.5800e-01, 4.2065e-01, 5.3710e-01, 7.4013e-02, 2.2130e-01,\n",
       "         1.5384e-01, 1.9019e-02, 4.6057e-01, 1.6725e-01, 3.2182e-01, 3.7431e-01,\n",
       "         9.2019e-01, 7.0618e-01, 9.9715e-01, 5.0181e-01, 2.0068e-02, 6.3303e-01,\n",
       "         7.7059e-01, 8.1246e-01, 5.1890e-02, 4.9120e-01, 9.8301e-01, 4.7896e-01,\n",
       "         1.9681e-01, 8.8707e-01, 3.3646e-01, 9.8086e-01, 9.6099e-01, 8.7634e-01,\n",
       "         4.3926e-01, 3.4998e-01, 6.0807e-01, 2.2183e-01, 2.7705e-01, 8.1425e-01,\n",
       "         7.4209e-01, 4.3409e-01, 8.9186e-01, 4.8214e-01, 8.0432e-01, 7.4560e-01,\n",
       "         7.4044e-01, 1.5486e-01, 2.1043e-01, 4.6064e-01, 7.7125e-01, 9.5188e-01,\n",
       "         9.1559e-02, 9.9149e-01]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src = torch.rand(3, d_model)\n",
    "src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pass it to the encoder to encode the input into three vectors of the same size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5273,  0.9600, -0.6093, -0.6646, -0.6597,  1.1945,  1.1089,  1.3499,\n",
       "          0.7104,  0.4301,  0.1325, -0.1241, -1.3403, -0.4634,  0.5402, -1.5561,\n",
       "         -1.1423, -1.1000,  1.1864, -0.0523,  0.0566, -1.8818,  0.5103, -1.0161,\n",
       "          0.0258,  1.9655, -0.1804,  0.6953,  0.8691,  0.5261,  0.4222, -0.2714,\n",
       "          2.0409, -0.4910, -1.7309, -0.4172,  0.0943, -0.6705, -1.8872, -0.9480,\n",
       "          1.6792, -0.0728,  0.3494,  0.2762,  1.2975,  0.4296, -1.6215,  0.3692,\n",
       "          0.2407,  0.9673],\n",
       "        [-0.6800, -1.5413,  1.1617,  0.1298,  0.8917,  0.5596, -0.2446,  1.0260,\n",
       "          1.0975, -0.1537,  0.1419, -0.6344, -0.5571, -2.0732, -0.6357, -2.5549,\n",
       "         -0.2038, -0.4180,  0.5532, -1.2640, -0.7922,  0.1034,  1.1163, -1.3783,\n",
       "          0.3447,  1.5014,  0.2621,  1.3420, -0.9427,  2.0574,  2.2395, -0.1421,\n",
       "          0.4107, -1.0464,  0.1306, -1.0431, -0.4672,  1.3655, -0.4376, -1.2799,\n",
       "         -0.3030, -0.0617, -0.2782,  0.4590,  1.1700,  0.3086, -0.6566, -0.2776,\n",
       "          0.9024,  0.7927],\n",
       "        [-1.5115, -0.6116, -0.6915,  0.6492, -2.0943, -0.0663, -0.5371,  0.5388,\n",
       "         -0.5677, -0.5960, -1.3102, -0.7172,  0.6841, -1.0244,  1.2744, -1.0146,\n",
       "         -1.4678, -2.0353,  1.1266,  0.4682, -0.9071, -0.0146,  0.5453, -0.6351,\n",
       "         -0.1413,  1.5200,  0.5101,  2.4381,  0.8322,  1.3915,  1.0349, -0.0587,\n",
       "          0.8393, -1.6786, -0.4458, -0.5650, -0.1012, -0.0848,  1.5142, -0.7396,\n",
       "          1.2607,  0.5860,  0.4594, -0.4565,  0.0995, -0.3144,  0.0693,  1.4119,\n",
       "         -0.0825,  1.2173]], grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = transformer_encoder(src)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a statement that will compute the mean of these embeddings. You will use the `mean(dim)` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.2396, -0.3976, -0.0464,  0.0381, -0.6207,  0.5626,  0.1091,  0.9716,\n",
       "         0.4134, -0.1065, -0.3453, -0.4919, -0.4044, -1.1870,  0.3929, -1.7085,\n",
       "        -0.9380, -1.1844,  0.9554, -0.2827, -0.5476, -0.5976,  0.7240, -1.0098,\n",
       "         0.0764,  1.6623,  0.1973,  1.4918,  0.2528,  1.3250,  1.2322, -0.1574,\n",
       "         1.0970, -1.0720, -0.6820, -0.6751, -0.1580,  0.2034, -0.2702, -0.9892,\n",
       "         0.8789,  0.1505,  0.1769,  0.0929,  0.8557,  0.1413, -0.7363,  0.5012,\n",
       "         0.3535,  0.9924], grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write your code\n",
    "out_mean = out.mean(dim=0)\n",
    "out_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have understood the first steps of SBERT, we can implement them in a class.\n",
    "\n",
    "In the next cell, write the `forward()` method that takes the two sentences as input in the form of two `LongTensor` of indices.\n",
    "1. Extract their embeddings from the GloVe embedding matrix.\n",
    "2. Encode them with the transformer, and \n",
    "3. Compute their respective mean. You will call these vectors $\\mathbf{u}$ and $\\mathbf{v}$\n",
    "4. Return these two vectors of means. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the forward method\n",
    "class SBERT(nn.Module):\n",
    "    def __init__(self, nbr_classes, glove, d_model=50, nhead=5, num_layers=4):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embeddings = nn.Embedding(\n",
    "            glove.size()[0],\n",
    "            glove.size()[1],\n",
    "            padding_idx=0).from_pretrained(glove)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model, nhead)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            self.encoder_layer, num_layers=num_layers)\n",
    "        # We do not use this last line for now\n",
    "        self.fc = nn.Linear(3 * d_model, nbr_classes)\n",
    "\n",
    "    def forward(self, u, v):\n",
    "        u = self.embeddings(u)\n",
    "        v = self.embeddings(v)\n",
    "        u = self.transformer_encoder(u)\n",
    "        v = self.transformer_encoder(v)\n",
    "        u = u.mean(dim=0)\n",
    "        v = v.mean(dim=0)\n",
    "        return u, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert = SBERT(3, glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SBERT(\n",
       "  (embeddings): Embedding(400003, 50)\n",
       "  (encoder_layer): TransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=50, out_features=50, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=50, out_features=2048, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=2048, out_features=50, bias=True)\n",
       "    (norm1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=50, out_features=50, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=50, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=50, bias=True)\n",
       "        (norm1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=150, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbert.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([   10,   902,    16,    10,  2870, 11073,    77,    10,  2324,   138,\n",
       "          7353,     5]),\n",
       " tensor([  10,  902,   17,  791,   29, 2870,   13,   10,  994,    5]),\n",
       " tensor(2))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, v = sbert(dataset[0][0], dataset[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 6.8417e-03,  1.2471e+00, -6.0113e-01,  6.4204e-01,  1.4145e-01,\n",
       "         -1.6292e-01, -1.1143e+00, -3.0791e-02,  1.2481e+00, -1.2497e+00,\n",
       "          5.9703e-01,  1.3712e-02, -1.8901e+00,  1.2502e+00,  5.2542e-01,\n",
       "          6.1148e-01,  2.0110e+00,  5.3191e-04,  7.4153e-02, -1.7274e-01,\n",
       "          8.0745e-01,  1.0392e+00, -1.4829e+00, -1.4686e+00, -6.3498e-01,\n",
       "         -2.1219e+00,  2.6609e-01, -2.9389e-01, -1.0923e+00, -2.9799e-01,\n",
       "          1.5527e+00, -1.0035e+00,  8.7872e-01, -3.7961e-01,  1.2611e+00,\n",
       "         -2.4842e-01,  1.1421e+00, -2.3525e-01, -1.0649e+00, -1.3096e+00,\n",
       "          1.4559e-01, -8.3565e-01,  8.8376e-01, -3.7018e-01,  5.9967e-01,\n",
       "         -2.4761e-01,  1.1052e+00,  2.7283e-02,  9.4663e-01, -7.1568e-01],\n",
       "        grad_fn=<MeanBackward1>),\n",
       " tensor([ 0.0524,  1.4210, -0.8081,  0.2262,  0.1673,  0.0494, -1.3689, -0.4148,\n",
       "          1.0811, -1.1450,  0.9079,  0.2128, -1.8392,  0.8040,  0.2362,  0.5926,\n",
       "          2.2157, -0.1631,  0.2221, -0.5283,  0.6507,  0.8830, -1.2024, -1.0717,\n",
       "         -0.8839, -2.2133,  0.6436, -0.3635, -1.4111, -0.3488,  1.4200, -1.1622,\n",
       "          0.5445, -0.6413,  1.4409,  0.2924,  1.1040, -0.0645, -0.9800, -1.6282,\n",
       "          0.4256, -0.7312,  0.6124, -0.2770,  0.7952, -0.0471,  0.7686,  0.5121,\n",
       "          1.2116, -0.1996], grad_fn=<MeanBackward1>))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u, v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the code that concatenate `u`, `v`, and `|u-v|`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6.8417e-03,  1.2471e+00, -6.0113e-01,  6.4204e-01,  1.4145e-01,\n",
       "        -1.6292e-01, -1.1143e+00, -3.0791e-02,  1.2481e+00, -1.2497e+00,\n",
       "         5.9703e-01,  1.3712e-02, -1.8901e+00,  1.2502e+00,  5.2542e-01,\n",
       "         6.1148e-01,  2.0110e+00,  5.3191e-04,  7.4153e-02, -1.7274e-01,\n",
       "         8.0745e-01,  1.0392e+00, -1.4829e+00, -1.4686e+00, -6.3498e-01,\n",
       "        -2.1219e+00,  2.6609e-01, -2.9389e-01, -1.0923e+00, -2.9799e-01,\n",
       "         1.5527e+00, -1.0035e+00,  8.7872e-01, -3.7961e-01,  1.2611e+00,\n",
       "        -2.4842e-01,  1.1421e+00, -2.3525e-01, -1.0649e+00, -1.3096e+00,\n",
       "         1.4559e-01, -8.3565e-01,  8.8376e-01, -3.7018e-01,  5.9967e-01,\n",
       "        -2.4761e-01,  1.1052e+00,  2.7283e-02,  9.4663e-01, -7.1568e-01,\n",
       "         5.2396e-02,  1.4210e+00, -8.0805e-01,  2.2615e-01,  1.6734e-01,\n",
       "         4.9434e-02, -1.3689e+00, -4.1477e-01,  1.0811e+00, -1.1450e+00,\n",
       "         9.0785e-01,  2.1278e-01, -1.8392e+00,  8.0397e-01,  2.3623e-01,\n",
       "         5.9258e-01,  2.2157e+00, -1.6314e-01,  2.2214e-01, -5.2835e-01,\n",
       "         6.5067e-01,  8.8301e-01, -1.2024e+00, -1.0717e+00, -8.8394e-01,\n",
       "        -2.2133e+00,  6.4365e-01, -3.6352e-01, -1.4111e+00, -3.4875e-01,\n",
       "         1.4200e+00, -1.1622e+00,  5.4451e-01, -6.4133e-01,  1.4409e+00,\n",
       "         2.9240e-01,  1.1040e+00, -6.4471e-02, -9.8003e-01, -1.6282e+00,\n",
       "         4.2557e-01, -7.3123e-01,  6.1241e-01, -2.7701e-01,  7.9519e-01,\n",
       "        -4.7079e-02,  7.6862e-01,  5.1213e-01,  1.2116e+00, -1.9965e-01,\n",
       "         4.5555e-02,  1.7386e-01,  2.0692e-01,  4.1589e-01,  2.5887e-02,\n",
       "         2.1236e-01,  2.5461e-01,  3.8398e-01,  1.6701e-01,  1.0469e-01,\n",
       "         3.1082e-01,  1.9907e-01,  5.0816e-02,  4.4625e-01,  2.8919e-01,\n",
       "         1.8894e-02,  2.0473e-01,  1.6367e-01,  1.4799e-01,  3.5560e-01,\n",
       "         1.5678e-01,  1.5618e-01,  2.8052e-01,  3.9686e-01,  2.4897e-01,\n",
       "         9.1388e-02,  3.7756e-01,  6.9629e-02,  3.1877e-01,  5.0765e-02,\n",
       "         1.3266e-01,  1.5870e-01,  3.3421e-01,  2.6173e-01,  1.7976e-01,\n",
       "         5.4083e-01,  3.8112e-02,  1.7078e-01,  8.4901e-02,  3.1857e-01,\n",
       "         2.7997e-01,  1.0442e-01,  2.7135e-01,  9.3166e-02,  1.9552e-01,\n",
       "         2.0053e-01,  3.3659e-01,  4.8485e-01,  2.6499e-01,  5.1603e-01],\n",
       "       grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write your code\n",
    "diff = u - v\n",
    "diff = abs(diff)\n",
    "concat_tensor = torch.cat((u, v, diff), dim=0)\n",
    "concat_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to add the logistic regression head. Complement the `forward()` method so that it concatenates, $\\mathbf{u}$, $\\mathbf{v}$, and $\\mathbf{|u - v|}$ and outputs three classes. You have only two lines to add."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the forward() method\n",
    "class SBERT(nn.Module):\n",
    "    def __init__(self, nbr_classes, glove, d_model=50, nhead=5, num_layers=4):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embeddings = nn.Embedding(glove.size()[0],\n",
    "                                       glove.size()[1],\n",
    "                                       padding_idx=0).from_pretrained(glove)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model, nhead)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            self.encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(3 * d_model, nbr_classes)\n",
    "\n",
    "    def forward(self, u, v):\n",
    "        u = self.embeddings(u)\n",
    "        v = self.embeddings(v)\n",
    "        u = self.transformer_encoder(u)\n",
    "        v = self.transformer_encoder(v)\n",
    "        u = u.mean(dim=0)\n",
    "        v = v.mean(dim=0)\n",
    "        diff = abs(u - v)\n",
    "        x = torch.cat((u, v, diff), dim=0)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert = SBERT(3, glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SBERT(\n",
       "  (embeddings): Embedding(400003, 50)\n",
       "  (encoder_layer): TransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=50, out_features=50, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=50, out_features=2048, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=2048, out_features=50, bias=True)\n",
       "    (norm1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=50, out_features=50, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=50, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=50, bias=True)\n",
       "        (norm1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=150, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbert.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([   10,   902,    16,    10,  2870, 11073,    77,    10,  2324,   138,\n",
       "          7353,     5]),\n",
       " tensor([  10,  902,   17,  791,   29, 2870,   13,   10,  994,    5]),\n",
       " tensor(2))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the complete model and we have three outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.5493,  0.1353, -0.2695], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbert(dataset[0][0], dataset[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training SBERT\n",
    "We now train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SBERT(\n",
       "  (embeddings): Embedding(400003, 50)\n",
       "  (encoder_layer): TransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=50, out_features=50, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=50, out_features=2048, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=2048, out_features=50, bias=True)\n",
       "    (norm1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=50, out_features=50, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=50, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=50, bias=True)\n",
       "        (norm1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=150, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbert.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()    # cross entropy loss\n",
    "optimizer = torch.optim.Adam(sbert.parameters(), lr=0.00002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([   10,   902,    16,    10,  2870, 11073,    77,    10,  2324,   138,\n",
       "          7353,     5]),\n",
       " tensor([  10,  902,   17,  791,   29, 2870,   13,   10,  994,    5]),\n",
       " tensor(2))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the training loop. You will process one sample at a time to simplify it i.e. no batch. Record the training loss.\n",
    "\n",
    "A better design would use a `Dataset` object. We will see this construct in the last laboratory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 5493/5493 [01:38<00:00, 55.54it/s]\n",
      "100%|███████████████████████████████████████| 5493/5493 [01:38<00:00, 56.02it/s]\n",
      "100%|███████████████████████████████████████| 5493/5493 [01:36<00:00, 56.99it/s]\n",
      "100%|███████████████████████████████████████| 5493/5493 [01:41<00:00, 54.29it/s]\n",
      "100%|███████████████████████████████████████| 5493/5493 [01:46<00:00, 51.42it/s]\n"
     ]
    }
   ],
   "source": [
    "# Write your code\n",
    "ce_train_loss = []\n",
    "for epoch in range(5):\n",
    "    loss_train = 0\n",
    "    for data in tqdm(dataset, ncols=80):\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = sbert(data[0], data[1])\n",
    "        loss = loss_fn(y_pred.view(1, -1), data[2].view(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_train += loss.item()\n",
    "    ce_train_loss.append(loss_train / len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(range(len(ce_train_loss)),\n",
    "            ce_train_loss, c='b', marker='x')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Cross entropy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(sbert, 'sbert_mini.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_saved = torch.load('sbert_mini.pt', weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  10,  902,   17,  791,   29, 2870,   13,   10,  994,    5])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.9504, -0.4566,  1.6806], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbert(dataset[0][0], dataset[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write a function to apply the model to a sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sbert(model, token_indices):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        v = model.embeddings(token_indices)\n",
    "        v = model.transformer_encoder(v).mean(dim=0)\n",
    "        return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5.5014e-01,  2.2175e+00,  1.0943e-02, -1.5194e+00,  2.1388e-01,\n",
       "         1.6797e+00,  1.1322e+00,  5.0665e-01, -3.5126e-01,  2.0733e+00,\n",
       "        -6.5628e-01, -4.8998e-01,  1.0700e-01, -2.9056e-01,  6.3894e-01,\n",
       "        -2.0036e-01, -8.7259e-01, -4.8582e-02,  5.5429e-01,  7.8383e-01,\n",
       "        -7.3428e-01, -1.4809e+00, -1.1853e+00, -8.5416e-01,  5.3131e-01,\n",
       "         4.9299e-01, -1.7176e-01,  1.6508e-01, -1.5976e+00, -2.3696e-01,\n",
       "         6.2138e-01, -1.1657e-04,  5.9505e-01, -9.5836e-02, -1.0099e+00,\n",
       "         3.3183e-01, -1.2716e-01,  1.3237e+00,  6.1533e-01, -1.0503e-01,\n",
       "        -1.9033e-01,  1.2508e-01, -2.2741e-01, -6.9092e-01,  3.6271e-01,\n",
       "        -5.1588e-01, -6.7532e-01, -1.0130e+00, -9.1094e-01,  6.1061e-01])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_sbert(sbert, dataset[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'entailment', 1: 'contradiction', 2: 'neutral'}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the model to all our pairs and, for each pair, we compute the cosine of the resulting embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 5493/5493 [00:19<00:00, 280.58it/s]\n"
     ]
    }
   ],
   "source": [
    "cos_sim = {'entailment': 0.0, 'neutral': 0.0, 'contradiction': 0.0}\n",
    "cnt = {'entailment': 0.0, 'neutral': 0.0, 'contradiction': 0.0}\n",
    "sbert.eval()\n",
    "\n",
    "for data in tqdm(dataset, ncols=80):\n",
    "    cos_val = compute_cosine(\n",
    "        encode_sbert(sbert, data[0]),\n",
    "        encode_sbert(sbert, data[1]))\n",
    "    cos_sim[idx2label[data[2].item()]] += cos_val\n",
    "    cnt[idx2label[data[2].item()]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'entailment': tensor(1250.7367),\n",
       "  'neutral': tensor(940.1052),\n",
       "  'contradiction': tensor(755.0894)},\n",
       " {'entailment': 1839.0, 'neutral': 1821.0, 'contradiction': 1833.0})"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim, cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entailment': tensor(0.6801),\n",
       " 'neutral': tensor(0.5163),\n",
       " 'contradiction': tensor(0.4119)}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for key in cos_sim.keys():\n",
    "    cos_sim[key] /= cnt[key]\n",
    "cos_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying the Embedder to Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"The weather is lovely today.\",\n",
    "    \"It's so sunny outside!\",\n",
    "    \"He drove to the stadium.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the', 'weather', 'is', 'lovely', 'today', '.'],\n",
       " ['it', \"'\", 's', 'so', 'sunny', 'outside', '!'],\n",
       " ['he', 'drove', 'to', 'the', 'stadium', '.']]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sents = [tokenize(sent, pattern) for sent in sentences]\n",
    "tokenized_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([    3,  1623,    17, 11130,   376,     5]),\n",
       " tensor([  23,   60, 1537,  103, 9328,  590,  808]),\n",
       " tensor([  21, 3189,    7,    3, 1355,    5])]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexed_sents = [convert_symbols(sent, token2idx) for sent in tokenized_sents]\n",
    "indexed_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([-1.8848e-01,  1.1982e+00,  2.5010e-02,  3.9504e-01, -1.0252e+00,\n",
       "         -1.5856e-01,  5.4349e-01, -3.9641e-01,  1.5055e+00,  1.6015e-01,\n",
       "         -6.6313e-02,  7.4331e-01, -2.3850e-01, -7.7391e-01,  3.5829e-01,\n",
       "          5.6133e-01,  1.1988e+00, -4.9377e-01, -5.9495e-01,  1.9928e-03,\n",
       "          1.0307e+00,  1.9247e+00, -1.1480e+00, -1.9186e+00, -3.0253e-01,\n",
       "          3.2228e-01, -9.7055e-01,  1.5588e-01, -9.2422e-01,  8.3377e-01,\n",
       "          1.1448e+00, -9.1864e-02,  2.1595e+00,  2.7171e-01,  6.3378e-01,\n",
       "         -4.5301e-01, -7.9965e-01, -3.1435e-01, -3.2487e-01,  3.0877e-01,\n",
       "         -3.9673e-01,  3.3833e-02,  4.5688e-01, -3.3914e-01,  2.7871e-01,\n",
       "         -1.7331e+00, -5.1257e-01, -1.7199e+00, -1.0865e+00,  7.6033e-01]),\n",
       " tensor([-5.0900e-01,  2.4083e-01,  7.9409e-02,  8.9291e-01, -4.0303e-01,\n",
       "         -7.8569e-01, -1.1791e-01, -7.7999e-01,  1.6054e+00, -6.1590e-01,\n",
       "         -8.8663e-01,  5.6273e-01,  1.3995e-01, -2.1909e-01,  4.2992e-01,\n",
       "          4.9234e-01,  1.0839e+00, -1.4442e-01,  1.5873e-02, -2.1428e-01,\n",
       "          5.0621e-01,  2.3433e+00, -7.1946e-01, -1.7143e+00, -7.1824e-01,\n",
       "          5.0655e-01, -7.4203e-01, -3.1118e-01, -4.6943e-01, -1.6433e-01,\n",
       "          6.6193e-01,  4.1836e-01,  1.9198e+00,  7.1855e-01,  4.6546e-01,\n",
       "         -1.4207e+00, -1.8365e-01, -6.9953e-01, -4.4229e-01,  9.0878e-01,\n",
       "         -2.3933e-01,  7.9892e-01, -3.1134e-04, -2.3835e-02,  3.6348e-01,\n",
       "         -1.4426e+00,  2.6215e-01, -8.6682e-01, -1.0383e+00,  4.7528e-01]),\n",
       " tensor([-0.1292,  0.4831,  0.1904, -0.3509, -0.4716,  0.4178, -0.3675, -0.0957,\n",
       "          0.1345,  1.8292,  0.2130,  0.4406,  0.7516, -1.2819,  0.3700,  0.8693,\n",
       "         -1.0632,  0.5094,  1.2189,  0.6152, -1.7148, -1.6339, -0.8572, -1.5045,\n",
       "          1.0447,  0.5748, -0.6927, -0.1515, -1.6592,  1.0232, -0.1039,  0.4662,\n",
       "          0.9539,  0.3609, -0.3928,  0.5798,  0.1505,  1.3299,  0.6945,  1.2438,\n",
       "         -0.6537,  1.0068, -0.8417, -0.5603,  0.7749,  0.5380, -1.5479, -0.3736,\n",
       "         -1.9186, -0.4293])]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_sents = [encode_sbert(sbert, sent) for sent in indexed_sents]\n",
    "encoded_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.8269, 0.1600],\n",
       "        [0.8269, 1.0000, 0.0307],\n",
       "        [0.1600, 0.0307, 1.0000]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([compute_cosine(s1, s2)\n",
    "for s1 in encoded_sents\n",
    "for s2 in encoded_sents]).reshape(len(sentences), len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turning in your assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now your are done with the program. To complete this assignment, you will write a report where you will:\n",
    "1. Write a short individual report on your program. I recommend that you use this structure for your report:\n",
    "      1. Objectives and dataset\n",
    "      2. Method and program structure, where you should outline your program and possibly describe difficult parts.\n",
    "      3. Results.\n",
    "      4. Conclusion.\n",
    "2. In Sect. _Method and program structure_, do not forget to:\n",
    "   * Summarize the baseline\n",
    "   * Summarize SBERT\n",
    "\n",
    "The whole report should be of 2 to 3 pages.\n",
    "\n",
    "Submit your report as well as your **notebook** (for archiving purposes) to Canvas: https://canvas.education.lu.se/. To write your report, use Latex. This will probably help you structure your text. You can use the Overleaf online editor (www.overleaf.com). You will then upload a PDF file in Canvas.\n",
    "\n",
    "The submission deadline is October 18, 2024."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
