{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment #5: A sentence embedder\n",
    "Author: Pierre Nugues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you will implement a sentence embedder simplified from Reimers and Gurevych's Sentence-BERT: https://arxiv.org/pdf/1908.10084. S-BERT is written in PyTorch and its code is available from GitHub: https://github.com/UKPLab/sentence-transformers\n",
    "\n",
    "The objectives of the assignment are to:\n",
    "* Write a program to embed sentences\n",
    "* Use neural networks with PyTorch\n",
    "* Write a short report of 2 to 3 pages to describe your program.\n",
    "\n",
    "Note: Should your machine be unable to train a model for the whole dataset, then use only a fraction of the dataset such as 10% or less. For this, use the `MINI_CORPUS` constant. See below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "We saw we can vectorize words using a dense representation. We can extend this to documents. This enables us to store the resulting vectors in databases and then use fast algorithms for paragraph or document comparisons such as Faiss: https://github.com/facebookresearch/faiss\n",
    "\n",
    "There are many document vectorization techniques and models are regularly benchmarked, see: https://huggingface.co/spaces/mteb/leaderboard. See also a list of available vector databases here https://db-engines.com/en/ranking/vector+dbms\n",
    "\n",
    "In this lab, you will program two techniques to vectorize documents into dense vectors. You will first implement a baseline technique and then a toy version of SBERT. SBERT is one of the earliest transformer-based document vectorization algorithm: _Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks_ by Reimers and Gurevych (2019) https://arxiv.org/pdf/1908.10084"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of the algorithms\n",
    "Read the Getting Started paragraph of https://github.com/UKPLab/sentence-transformers for an overview.\n",
    "\n",
    "Read the summary of the SBERT paper as well as Sections 1 and 3, _Introduction_ and _Model_. In the triplet objective function, an anchor is a start sample, the positive sample is close to the anchor, while the negative one is different. Considering a language detector, think of a sentence in Swedish as the anchor. A positive sample would be another sentence in Swedish and a negative one could be a sentence in English.\n",
    "\n",
    "In the _Method and program struture_ section of your report, you will summarize these sections in 10 to 15 lines. Note that a three-way softmax classifier is simply a logistic regression with three classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import regex as re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a reduced dataset for the development of your program with `MINI_CORPUS` set to `True`. Once your program is ready, you can train your model on the whole dataset (if you have the time). Set `MINI_CORPUS` to `False` then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "MINI_CORPUS = True  # Set the value to True when you develop the program\n",
    "MINI_PERCENTAGE = 0.01  # Percentage of the original dataset.\n",
    "# Depending on your machine, you may even use less than 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets: SNLI\n",
    "As dataset, you will use SNLI. SNLI consists of over 500,000 lines with the text of the pairs and their labels. The authors created the dataset by giving volunteers a sentence (the premise) and asking them to write a second sentence (the hypothesis) that is either definitely true\n",
    "(entailment), that might be true (neutral), or that is definitely false (contradiction).\n",
    "\n",
    "Read the dataset description from this URL https://nlp.stanford.edu/projects/snli/ and download it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please adjust the path to fit your machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('snli_1.0/snli_1.0/snli_1.0_train.jsonl', 'r') as f:\n",
    "    dataset_list = list(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_snli = []\n",
    "for json_str in dataset_list:\n",
    "    dataset_snli += [json.loads(json_str)]\n",
    "    # print(f\"result: {result}\")\n",
    "    # print(isinstance(result, dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sample with an agreement in the annotation. The final annotation is the gold label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'annotator_labels': ['entailment'],\n",
       " 'captionID': '3706019259.jpg#3',\n",
       " 'gold_label': 'entailment',\n",
       " 'pairID': '3706019259.jpg#3r2e',\n",
       " 'sentence1': 'A foreign family is walking along a dirt path next to the water.',\n",
       " 'sentence1_binary_parse': '( ( A ( foreign family ) ) ( ( is ( ( walking ( along ( a ( dirt path ) ) ) ) ( next ( to ( the water ) ) ) ) ) . ) )',\n",
       " 'sentence1_parse': '(ROOT (S (NP (DT A) (JJ foreign) (NN family)) (VP (VBZ is) (VP (VBG walking) (PP (IN along) (NP (DT a) (NN dirt) (NN path))) (ADVP (JJ next) (PP (TO to) (NP (DT the) (NN water)))))) (. .)))',\n",
       " 'sentence2': 'A family of foreigners walks by the water.',\n",
       " 'sentence2_binary_parse': '( ( ( A family ) ( of foreigners ) ) ( ( walks ( by ( the water ) ) ) . ) )',\n",
       " 'sentence2_parse': '(ROOT (S (NP (NP (DT A) (NN family)) (PP (IN of) (NP (NNS foreigners)))) (VP (VBZ walks) (PP (IN by) (NP (DT the) (NN water)))) (. .)))'}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_snli[300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sample with no agreement in the annotation. The gold label is `_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'annotator_labels': ['contradiction', 'contradiction', 'neutral', 'neutral'],\n",
       " 'captionID': '2677109430.jpg#2',\n",
       " 'gold_label': '-',\n",
       " 'pairID': '2677109430.jpg#2r1c',\n",
       " 'sentence1': 'A small group of church-goers watch a choir practice.',\n",
       " 'sentence1_binary_parse': '( ( ( A ( small group ) ) ( of church-goers ) ) ( ( watch ( a ( choir practice ) ) ) . ) )',\n",
       " 'sentence1_parse': '(ROOT (S (NP (NP (DT A) (JJ small) (NN group)) (PP (IN of) (NP (NNS church-goers)))) (VP (VBP watch) (NP (DT a) (NN choir) (NN practice))) (. .)))',\n",
       " 'sentence2': 'A choir performs in front of packed crowd.',\n",
       " 'sentence2_binary_parse': '( ( A choir ) ( ( performs ( in ( front ( of ( packed crowd ) ) ) ) ) . ) )',\n",
       " 'sentence2_parse': '(ROOT (S (NP (DT A) (NN choir)) (VP (VBZ performs) (PP (IN in) (NP (NP (NN front)) (PP (IN of) (NP (JJ packed) (NN crowd)))))) (. .)))'}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_snli[145]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove all the samples that have no agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_str = []\n",
    "for sample in dataset_snli:\n",
    "    s1 = sample['sentence1']\n",
    "    s2 = sample['sentence2']\n",
    "    label = sample['gold_label']\n",
    "    if label != '-':\n",
    "        dataset_str += [(s1, s2, label)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "549367"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('A person on a horse jumps over a broken down airplane.',\n",
       " 'A person is training his horse for a competition.',\n",
       " 'neutral')"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('A person on a horse jumps over a broken down airplane.',\n",
       " 'A person is at a diner, ordering an omelette.',\n",
       " 'contradiction')"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_str[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('A person on a horse jumps over a broken down airplane.',\n",
       " 'A person is outdoors, on a horse.',\n",
       " 'entailment')"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_str[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MINI_CORPUS:\n",
    "    new_size = int(len(dataset_str) * MINI_PERCENTAGE)\n",
    "    dataset_str = dataset_str[:new_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5493"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets: GloVe\n",
    "\n",
    "You will first implement a baseline, an easy technique that serves as comparison for more elaborate ones. \n",
    "\n",
    "In Sect. 4.1 and Table 1 the authors proposed a baseline technique for computing a semantic\n",
    "textual similarity between two sentences that uses GloVe embeddings. Describe this technique in 5 to 10 lines in the _Method and program struture_ section. You will create a *Baseline* subsection for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe\n",
    "You will use a list of pretrained word embeddings to implement the baseline and GloVe is one such vector lists. GloVe is available in different dimensionalities (50, 100, 200, 300) and vocabulary sizes (400,000 words, 1.2M, 2.4M). \n",
    "\n",
    "Download the GloVe 6B embeddings from https://nlp.stanford.edu/projects/glove/, uncompress it, and keep the `glove.6B.50d.txt` file of 400,000 words with 50-dimensional vectors.\n",
    "\n",
    "Please adjust your path to read the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_file = 'GloVe data/glove.6B.50d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_embeddings(file):\n",
    "    \"\"\"\n",
    "    Return the embeddings in the from of a dictionary\n",
    "    :param file:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    embeddings = {}\n",
    "    glove = open(file, encoding='utf8')\n",
    "    for line in glove:\n",
    "        values = line.strip().split()\n",
    "        word = values[0]\n",
    "        vector = torch.tensor(\n",
    "            list(map(float, values[1:])), dtype=torch.float32)\n",
    "        embeddings[word] = vector\n",
    "    glove.close()\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = read_embeddings(embedding_file)\n",
    "embedded_words = sorted(list(embeddings.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4.1800e-01,  2.4968e-01, -4.1242e-01,  1.2170e-01,  3.4527e-01,\n",
       "        -4.4457e-02, -4.9688e-01, -1.7862e-01, -6.6023e-04, -6.5660e-01,\n",
       "         2.7843e-01, -1.4767e-01, -5.5677e-01,  1.4658e-01, -9.5095e-03,\n",
       "         1.1658e-02,  1.0204e-01, -1.2792e-01, -8.4430e-01, -1.2181e-01,\n",
       "        -1.6801e-02, -3.3279e-01, -1.5520e-01, -2.3131e-01, -1.9181e-01,\n",
       "        -1.8823e+00, -7.6746e-01,  9.9051e-02, -4.2125e-01, -1.9526e-01,\n",
       "         4.0071e+00, -1.8594e-01, -5.2287e-01, -3.1681e-01,  5.9213e-04,\n",
       "         7.4449e-03,  1.7778e-01, -1.5897e-01,  1.2041e-02, -5.4223e-02,\n",
       "        -2.9871e-01, -1.5749e-01, -3.4758e-01, -4.5637e-02, -4.4251e-01,\n",
       "         1.8785e-01,  2.7849e-03, -1.8411e-01, -1.1514e-01, -7.8581e-01])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model = next(iter(embeddings.values())).size()[0]\n",
    "d_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read all the words in GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_words = []\n",
    "glove = []\n",
    "for word, vector in embeddings.items():\n",
    "    glove_words += [word]\n",
    "    glove += [vector]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', ',', '.', 'of', 'to', 'and', 'in', 'a', '\"', \"'s\"]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we create a tensor with the vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = torch.stack(glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4.1800e-01,  2.4968e-01, -4.1242e-01,  1.2170e-01,  3.4527e-01,\n",
       "        -4.4457e-02, -4.9688e-01, -1.7862e-01, -6.6023e-04, -6.5660e-01,\n",
       "         2.7843e-01, -1.4767e-01, -5.5677e-01,  1.4658e-01, -9.5095e-03,\n",
       "         1.1658e-02,  1.0204e-01, -1.2792e-01, -8.4430e-01, -1.2181e-01,\n",
       "        -1.6801e-02, -3.3279e-01, -1.5520e-01, -2.3131e-01, -1.9181e-01,\n",
       "        -1.8823e+00, -7.6746e-01,  9.9051e-02, -4.2125e-01, -1.9526e-01,\n",
       "         4.0071e+00, -1.8594e-01, -5.2287e-01, -3.1681e-01,  5.9213e-04,\n",
       "         7.4449e-03,  1.7778e-01, -1.5897e-01,  1.2041e-02, -5.4223e-02,\n",
       "        -2.9871e-01, -1.5749e-01, -3.4758e-01, -4.5637e-02, -4.4251e-01,\n",
       "         1.8785e-01,  2.7849e-03, -1.8411e-01, -1.1514e-01, -7.8581e-01])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove[0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of word embeddings (400,000) and their dimension (50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([400000, 50])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model = glove.size()[1]\n",
    "d_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reserve three special symbols: padding, unknown, and the first classification token of BERT. See the lecture on transformers for a clarification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "specials = ['[PAD]', '[UNK]', '[CLS]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[PAD]', '[UNK]', '[CLS]', 'the', ',', '.', 'of', 'to', 'and', 'in']"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_words = specials + glove_words\n",
    "glove_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add the vectors for the special tokens to our tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = torch.vstack((torch.zeros((3, d_model)), glove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([400003, 50])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0.])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4.1800e-01,  2.4968e-01, -4.1242e-01,  1.2170e-01,  3.4527e-01,\n",
       "        -4.4457e-02, -4.9688e-01, -1.7862e-01, -6.6023e-04, -6.5660e-01,\n",
       "         2.7843e-01, -1.4767e-01, -5.5677e-01,  1.4658e-01, -9.5095e-03,\n",
       "         1.1658e-02,  1.0204e-01, -1.2792e-01, -8.4430e-01, -1.2181e-01,\n",
       "        -1.6801e-02, -3.3279e-01, -1.5520e-01, -2.3131e-01, -1.9181e-01,\n",
       "        -1.8823e+00, -7.6746e-01,  9.9051e-02, -4.2125e-01, -1.9526e-01,\n",
       "         4.0071e+00, -1.8594e-01, -5.2287e-01, -3.1681e-01,  5.9213e-04,\n",
       "         7.4449e-03,  1.7778e-01, -1.5897e-01,  1.2041e-02, -5.4223e-02,\n",
       "        -2.9871e-01, -1.5749e-01, -3.4758e-01, -4.5637e-02, -4.4251e-01,\n",
       "         1.8785e-01,  2.7849e-03, -1.8411e-01, -1.1514e-01, -7.8581e-01])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove[3, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "### Tokenization\n",
    "You will now tokenize the sentences\n",
    "\n",
    "Write a regular expression that tokenizes the words, numbers, and punctuation. Use Unicode classes. Note that a punctuation is a single symbol while the words and numbers are sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "pattern = '\\p{L}+|\\p{N}+|\\p{Punct}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a tokenization function that takes a string as input and results a list of tokens. Set the string in lower case by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "def tokenize(sentence, pattern, lc=True):\n",
    "    if lc:\n",
    "        sentence = sentence.lower()\n",
    "    tokens = re.findall(pattern, sentence)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'person',\n",
       " 'on',\n",
       " 'a',\n",
       " 'horse',\n",
       " 'jumps',\n",
       " 'over',\n",
       " 'a',\n",
       " 'broken',\n",
       " 'down',\n",
       " 'airplane',\n",
       " '.']"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(dataset_str[0][0], pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a code that, for each sample of your dataset, builds a triple consisting of:\n",
    "1. The first tokenized sentence, \n",
    "2. The second one, and \n",
    "3. The class\n",
    "\n",
    "Build a list of all these triples to represent your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "dataset_tokens = []\n",
    "for s1, s2, label in dataset_str:\n",
    "    tokens1 = tokenize(s1, pattern)\n",
    "    tokens2 = tokenize(s2, pattern)\n",
    "    dataset_tokens += [(tokens1, tokens2, label)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['a',\n",
       "  'person',\n",
       "  'on',\n",
       "  'a',\n",
       "  'horse',\n",
       "  'jumps',\n",
       "  'over',\n",
       "  'a',\n",
       "  'broken',\n",
       "  'down',\n",
       "  'airplane',\n",
       "  '.'],\n",
       " ['a',\n",
       "  'person',\n",
       "  'is',\n",
       "  'training',\n",
       "  'his',\n",
       "  'horse',\n",
       "  'for',\n",
       "  'a',\n",
       "  'competition',\n",
       "  '.'],\n",
       " 'neutral')"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5493"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build token-to-index `token2idx` and index-to-token `idx2token` dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "token2idx = {word: i for i, word in enumerate(glove_words)}\n",
    "idx2token = {i: word for i, word in enumerate(glove_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 10)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2idx['the'], token2idx['a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('the', 'a')"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2token[3], idx2token[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect the set of all the labels (classes) from your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "\"\"\"\n",
    "labels_set = set()\n",
    "for s1, s2, label in dataset_tokens:\n",
    "    labels_set.add(label)\n",
    "labels = list(labels_set)\n",
    "\"\"\"\n",
    "\n",
    "labels = ['entailment', 'contradiction', 'neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['entailment', 'contradiction', 'neutral']"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build label-to-index `label2idx` and index-to-label `idx2label` dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "label2idx = {label: i for i, label in enumerate(labels)}\n",
    "idx2label = {i: label for i, label in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entailment': 0, 'contradiction': 1, 'neutral': 2}"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'entailment', 1: 'contradiction', 2: 'neutral'}"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function to convert:\n",
    "  * A list of tokens into a list of `LongTensor` indices and \n",
    "  * The class to a tensor. \n",
    "\n",
    "Your function should be able to handle two types: either a list or a string. The tokens are strored in a list and the class (label) is a string\n",
    "\n",
    "Note that an unknown token in GloVe should be mapped to the `UNK` symbol of index 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "def convert_symbols(symbols, symbol2idx):\n",
    "    if type(symbols) is str:\n",
    "        try:\n",
    "            idx = symbol2idx[symbols]\n",
    "        except KeyError:\n",
    "            idx = symbol2idx['[UNK]']\n",
    "        return torch.tensor(idx, dtype=torch.long)\n",
    "    else:\n",
    "        indices = []\n",
    "        for symbol in symbols:\n",
    "            try:\n",
    "                idx = symbol2idx[symbol]\n",
    "            except KeyError:\n",
    "                idx = symbol2idx['[UNK]']\n",
    "            indices.append(idx)\n",
    "        return torch.tensor(indices, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['a',\n",
       "  'person',\n",
       "  'on',\n",
       "  'a',\n",
       "  'horse',\n",
       "  'jumps',\n",
       "  'over',\n",
       "  'a',\n",
       "  'broken',\n",
       "  'down',\n",
       "  'airplane',\n",
       "  '.'],\n",
       " ['a',\n",
       "  'person',\n",
       "  'is',\n",
       "  'training',\n",
       "  'his',\n",
       "  'horse',\n",
       "  'for',\n",
       "  'a',\n",
       "  'competition',\n",
       "  '.'],\n",
       " 'neutral')"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_tokens[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert a list of tokens into a `LongTensor` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   10,   902,    16,    10,  2870, 11073,    77,    10,  2324,   138,\n",
       "         7353,     5])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_symbols(dataset_tokens[0][0], token2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  10,  902,   17,  791,   29, 2870,   13,   10,  994,    5])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_symbols(dataset_tokens[0][1], token2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neutral'"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_tokens[0][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert a label string into a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_symbols(dataset_tokens[0][2], label2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unknown tokens have the index 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'person', 'on', 'a', 'horsewww']"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize('a person on a horsewww', pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 10, 902,  16,  10,   1,   1])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_symbols(tokenize('a person on a horsewww wxwx', pattern), token2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the tokens and labels in your dataset by their indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "dataset = []\n",
    "for s1, s2, label in dataset_tokens:\n",
    "    x1 = convert_symbols(s1, token2idx)\n",
    "    x2 = convert_symbols(s2, token2idx)\n",
    "    y = convert_symbols(label, label2idx)\n",
    "    dataset += [(x1, x2, y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([   10,   902,    16,    10,  2870, 11073,    77,    10,  2324,   138,\n",
       "          7353,     5]),\n",
       " tensor([    10,    902,     17,     25,     10,  19304,      4,   7490,     32,\n",
       "         119031,      5]),\n",
       " tensor(1))"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch `Embedding` class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now store the GloVe vectors in a PyTorch `Embedding` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embs = nn.Embedding(glove.size()[0],\n",
    "glove.size()[1],\n",
    "padding_idx=0).from_pretrained(glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We access the embedding for _the_ with its index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.1800e-01,  2.4968e-01, -4.1242e-01,  1.2170e-01,  3.4527e-01,\n",
       "         -4.4457e-02, -4.9688e-01, -1.7862e-01, -6.6023e-04, -6.5660e-01,\n",
       "          2.7843e-01, -1.4767e-01, -5.5677e-01,  1.4658e-01, -9.5095e-03,\n",
       "          1.1658e-02,  1.0204e-01, -1.2792e-01, -8.4430e-01, -1.2181e-01,\n",
       "         -1.6801e-02, -3.3279e-01, -1.5520e-01, -2.3131e-01, -1.9181e-01,\n",
       "         -1.8823e+00, -7.6746e-01,  9.9051e-02, -4.2125e-01, -1.9526e-01,\n",
       "          4.0071e+00, -1.8594e-01, -5.2287e-01, -3.1681e-01,  5.9213e-04,\n",
       "          7.4449e-03,  1.7778e-01, -1.5897e-01,  1.2041e-02, -5.4223e-02,\n",
       "         -2.9871e-01, -1.5749e-01, -3.4758e-01, -4.5637e-02, -4.4251e-01,\n",
       "          1.8785e-01,  2.7849e-03, -1.8411e-01, -1.1514e-01, -7.8581e-01]])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_embs(torch.LongTensor([3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: Mean of GloVe Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('A person on a horse jumps over a broken down airplane.',\n",
       " 'A person is training his horse for a competition.',\n",
       " 'neutral')"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1_str = dataset_str[0][0]\n",
    "s2_str = dataset_str[0][1]\n",
    "s3_str = dataset_str[0][2]\n",
    "s1_str, s2_str, s3_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([   10,   902,    16,    10,  2870, 11073,    77,    10,  2324,   138,\n",
       "          7353,     5]),\n",
       " tensor([  10,  902,   17,  791,   29, 2870,   13,   10,  994,    5]),\n",
       " tensor(2))"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1_idx = dataset[0][0]\n",
    "s2_idx = dataset[0][1]\n",
    "s3_idx = dataset[0][2]\n",
    "s1_idx, s2_idx, s3_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function that takes a list of indices and the GloVe embeddings as input and that computes the mean of the word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "def mean_embs(input_idx: torch.LongTensor, glove_embs: nn.Embedding) -> torch.tensor:\n",
    "    embs = glove_embs(input_idx)\n",
    "    return embs.mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([   10,   902,    16,    10,  2870, 11073,    77,    10,  2324,   138,\n",
       "          7353,     5]),\n",
       " tensor([  10,  902,   17,  791,   29, 2870,   13,   10,  994,    5]),\n",
       " tensor(2))"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1988,  0.1043,  0.0180, -0.0851,  0.5251,  0.4551, -0.4729, -0.0604,\n",
       "         0.1335, -0.1824, -0.1023, -0.2145, -0.3953,  0.3100,  0.3126, -0.1235,\n",
       "        -0.1631,  0.1271, -0.8334, -0.5111,  0.0911,  0.1766, -0.1190, -0.1795,\n",
       "         0.2117, -1.6935, -0.1754,  0.3900,  0.4590, -0.1137,  3.0905, -0.0358,\n",
       "        -0.2404,  0.2918,  0.1015, -0.0099,  0.2168,  0.1239,  0.1565, -0.2061,\n",
       "        -0.1449,  0.0871, -0.1085,  0.1992, -0.0306, -0.2125,  0.1155, -0.3489,\n",
       "         0.2139, -0.1993])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_embs(dataset[0][0], glove_embs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function to compute the cosine of two vectors. You will return `torch.tensor(0.0)` if one of the vectors is zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "def compute_cosine(v1: torch.tensor, v2: torch.tensor) -> torch.tensor:\n",
    "    if torch.equal(v1, torch.zeros_like(v1)) or torch.equal(v2, torch.zeros_like(v2)):\n",
    "        return torch.tensor(0.0)\n",
    "    return torch.dot(v1, v2) / (torch.norm(v1) * torch.norm(v2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.1988,  0.1043,  0.0180, -0.0851,  0.5251,  0.4551, -0.4729, -0.0604,\n",
       "          0.1335, -0.1824, -0.1023, -0.2145, -0.3953,  0.3100,  0.3126, -0.1235,\n",
       "         -0.1631,  0.1271, -0.8334, -0.5111,  0.0911,  0.1766, -0.1190, -0.1795,\n",
       "          0.2117, -1.6935, -0.1754,  0.3900,  0.4590, -0.1137,  3.0905, -0.0358,\n",
       "         -0.2404,  0.2918,  0.1015, -0.0099,  0.2168,  0.1239,  0.1565, -0.2061,\n",
       "         -0.1449,  0.0871, -0.1085,  0.1992, -0.0306, -0.2125,  0.1155, -0.3489,\n",
       "          0.2139, -0.1993]),\n",
       " tensor([ 1.0878e-01,  3.7231e-01, -4.7114e-01, -1.3591e-02,  5.1340e-01,\n",
       "          3.7193e-01, -4.4592e-01, -5.9900e-02,  2.5352e-01, -1.3076e-01,\n",
       "          1.6231e-01,  2.3146e-03, -3.6474e-01,  2.3840e-03,  3.4653e-01,\n",
       "         -2.1769e-01,  2.5946e-02,  3.9537e-01, -6.9517e-01, -3.4811e-01,\n",
       "         -4.9454e-02,  1.4977e-01, -1.2447e-01,  8.1851e-02,  6.2581e-02,\n",
       "         -1.8692e+00, -3.1502e-01, -7.4079e-02,  1.2478e-01, -1.6717e-02,\n",
       "          3.3707e+00,  8.7725e-02, -4.0180e-01, -1.8131e-01,  2.5315e-01,\n",
       "          1.7589e-01,  2.2877e-01,  4.3286e-01, -1.6315e-02, -3.6988e-01,\n",
       "         -2.8208e-02,  3.1538e-02, -2.4721e-01,  2.5963e-01,  1.3792e-02,\n",
       "         -2.6431e-01,  1.7081e-02, -2.0042e-01,  1.6257e-01,  2.1471e-01]))"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 = mean_embs(s1_idx, glove_embs)\n",
    "v2 = mean_embs(s2_idx, glove_embs)\n",
    "v1, v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50]), torch.Size([50]))"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1.size(), v2.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9426)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_cosine(v1, v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now compute the cosine of pairs for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5493/5493 [00:00<00:00, 9862.11it/s]\n"
     ]
    }
   ],
   "source": [
    "cos_sim = {'entailment': 0.0, 'neutral': 0.0, 'contradiction': 0.0}\n",
    "cnt = {'entailment': 0.0, 'neutral': 0.0, 'contradiction': 0.0}\n",
    "for data in tqdm(dataset):\n",
    "    cos_val = compute_cosine(\n",
    "        mean_embs(data[0], glove_embs),\n",
    "        mean_embs(data[1], glove_embs))\n",
    "    class_name = idx2label[data[2].item()]\n",
    "    cos_sim[class_name] += cos_val\n",
    "    cnt[class_name] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will comment these values in your report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entailment': tensor(0.9453),\n",
       " 'neutral': tensor(0.9385),\n",
       " 'contradiction': tensor(0.9298)}"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for key in cos_sim.keys():\n",
    "    cos_sim[key] /= cnt[key]\n",
    "cos_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SBERT: The Stacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now create the SBERT architecture and replicate the pipeline in Fig. 1 in the paper. In the next cells, we walk through the figure from the bottom to the top."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first layer builds an input consisting of two sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([   3,  360, 5453]), tensor([   3,  194,  368, 2929]))"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1 = torch.LongTensor(\n",
    "    list(map(lambda x: token2idx.get(x, 1), tokenize('the small cat', pattern))))\n",
    "p2 = torch.LongTensor(\n",
    "    list(map(lambda x: token2idx.get(x, 1), tokenize('the very big dog', pattern))))\n",
    "p1, p2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we have the BERT layer. Using PyTorch classes, create an encoder of four layers where each layer has five heads. You will use the classes `TransformerEncoderLayer` and `TransformerEncoder`. The dimensionality `d_model` is 50 as this is the size of GloVe vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oscar/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "# Write your code\n",
    "encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=5)\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerEncoder(\n",
       "  (layers): ModuleList(\n",
       "    (0-3): 4 x TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=50, out_features=50, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=50, out_features=2048, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=50, bias=True)\n",
       "      (norm1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now a BERT encoder. We associate each input index to an embedding. In the next cell, we create three embedding vectors correponding to three words.\n",
    "\n",
    "In the rest of the program, all our batches will have only one sample to eliminate the need for padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7716, 0.6775, 0.3091, 0.1023, 0.4207, 0.6062, 0.6722, 0.0992, 0.8912,\n",
       "         0.0508, 0.9853, 0.7219, 0.6409, 0.0844, 0.5023, 0.6966, 0.8174, 0.9556,\n",
       "         0.9897, 0.5374, 0.8000, 0.2696, 0.3359, 0.9742, 0.0075, 0.8029, 0.3048,\n",
       "         0.3859, 0.4881, 0.8104, 0.3600, 0.7872, 0.3976, 0.5542, 0.7531, 0.7262,\n",
       "         0.6742, 0.7672, 0.5975, 0.0255, 0.5896, 0.9107, 0.1240, 0.5323, 0.8801,\n",
       "         0.6971, 0.8697, 0.1972, 0.2163, 0.9700],\n",
       "        [0.4947, 0.6729, 0.8063, 0.4357, 0.2562, 0.4494, 0.0066, 0.2181, 0.6524,\n",
       "         0.1352, 0.8619, 0.6744, 0.4922, 0.0373, 0.2101, 0.7582, 0.0162, 0.9750,\n",
       "         0.6815, 0.5706, 0.1201, 0.7452, 0.0739, 0.1185, 0.0779, 0.9736, 0.1989,\n",
       "         0.4345, 0.5972, 0.1417, 0.2741, 0.8801, 0.8205, 0.1233, 0.6586, 0.7799,\n",
       "         0.9688, 0.4162, 0.2595, 0.8268, 0.3621, 0.5293, 0.8220, 0.3932, 0.6078,\n",
       "         0.8737, 0.7503, 0.7381, 0.1076, 0.2499],\n",
       "        [0.1829, 0.7084, 0.0791, 0.6505, 0.3601, 0.1560, 0.5487, 0.5880, 0.1233,\n",
       "         0.0457, 0.5144, 0.7252, 0.0209, 0.7198, 0.5337, 0.3900, 0.6301, 0.4871,\n",
       "         0.2504, 0.9131, 0.7704, 0.7381, 0.8919, 0.7993, 0.3081, 0.5560, 0.1185,\n",
       "         0.3999, 0.8325, 0.0170, 0.3119, 0.0071, 0.1518, 0.1086, 0.9057, 0.8715,\n",
       "         0.6156, 0.5868, 0.9997, 0.9259, 0.3903, 0.0573, 0.8372, 0.2755, 0.0522,\n",
       "         0.2283, 0.6009, 0.7159, 0.1956, 0.7763]])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src = torch.rand(3, d_model)\n",
    "src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pass it to the encoder to encode the input into three vectors of the same size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.5729e-01,  1.1526e+00, -1.1553e-01, -3.8913e-01,  2.9310e-01,\n",
       "          4.5190e-01, -2.8171e+00, -1.5183e+00,  1.5683e-01, -6.5355e-01,\n",
       "          7.3381e-01,  1.8806e+00, -1.2033e+00,  1.9567e-01, -9.2122e-01,\n",
       "          7.4245e-01,  6.3361e-02,  2.6430e+00,  1.0463e-01,  6.5122e-01,\n",
       "          5.7865e-01,  2.0885e-01, -1.9607e-01,  1.1369e+00, -1.3488e+00,\n",
       "         -4.2729e-01, -3.6619e-01, -1.5548e+00,  1.1850e+00,  1.0705e+00,\n",
       "         -2.4022e-01,  7.3302e-01, -1.6365e+00, -9.7959e-02,  2.1182e-01,\n",
       "         -5.2959e-01, -8.9395e-01,  1.2780e+00, -4.8385e-01, -5.1724e-01,\n",
       "         -2.1903e-01, -1.3043e+00,  8.1458e-01,  1.5961e+00, -2.7341e-01,\n",
       "          8.0547e-01, -7.3337e-02, -9.3128e-01, -8.3880e-01,  7.0552e-01],\n",
       "        [-2.8313e-01,  1.7113e+00,  1.4356e-01,  6.6436e-01,  7.6983e-01,\n",
       "          1.2013e+00, -2.8007e+00, -8.3534e-01, -1.3358e-01,  3.3019e-01,\n",
       "         -3.0811e-01,  1.4707e+00, -7.8564e-01,  6.4073e-02, -5.8276e-01,\n",
       "          8.6324e-01, -1.3837e+00,  1.6967e+00,  5.6309e-01,  8.0990e-01,\n",
       "         -9.1132e-02,  1.7621e+00, -1.8275e-01, -4.1431e-01, -8.8034e-01,\n",
       "         -1.0466e+00,  1.3446e-02, -1.4309e+00,  1.3918e+00, -5.5661e-02,\n",
       "          1.5703e-01,  1.5305e+00, -7.1975e-01, -5.2775e-01, -9.7188e-02,\n",
       "         -4.5128e-01, -4.5334e-01,  7.6310e-01, -7.5661e-01,  6.4471e-01,\n",
       "         -5.6857e-01, -2.4938e+00,  1.5867e+00,  1.0141e+00,  2.3816e-01,\n",
       "          2.4216e-03, -8.8240e-01, -1.2821e-01, -8.8122e-01, -2.1783e-01],\n",
       "        [ 1.4458e-02,  1.7590e+00, -6.6215e-01,  9.3918e-01,  9.9198e-01,\n",
       "          6.0876e-01, -2.3905e+00, -3.8022e-01, -6.4870e-02,  1.3232e-01,\n",
       "         -4.2380e-01,  1.6531e+00, -1.4120e+00,  1.0438e+00, -3.8290e-01,\n",
       "          3.1176e-01, -1.8270e-01,  8.2518e-01, -7.0036e-01,  1.0264e+00,\n",
       "         -3.3022e-01,  3.1074e-01,  1.1852e+00,  1.1734e+00, -7.8385e-01,\n",
       "         -6.2903e-01, -2.5338e-01, -1.9963e+00,  1.8372e+00,  2.1966e-01,\n",
       "          3.4447e-02, -5.1384e-01, -1.1230e+00, -7.7136e-01,  3.0945e-01,\n",
       "         -1.8268e-01, -3.0269e-01,  3.6133e-01,  6.8634e-01,  1.0599e+00,\n",
       "         -1.1988e+00, -2.5309e+00,  1.7426e+00,  1.2912e+00, -1.0142e+00,\n",
       "         -7.1908e-01, -1.5832e-01, -4.4217e-01,  4.5328e-02, -1.3348e-02]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = transformer_encoder(src)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a statement that will compute the mean of these embeddings. You will use the `mean(dim)` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-3.7127e-02,  1.5410e+00, -2.1137e-01,  4.0480e-01,  6.8497e-01,\n",
       "         7.5400e-01, -2.6694e+00, -9.1127e-01, -1.3874e-02, -6.3680e-02,\n",
       "         6.3320e-04,  1.6681e+00, -1.1337e+00,  4.3450e-01, -6.2896e-01,\n",
       "         6.3915e-01, -5.0102e-01,  1.7216e+00, -1.0883e-02,  8.2919e-01,\n",
       "         5.2432e-02,  7.6058e-01,  2.6878e-01,  6.3201e-01, -1.0043e+00,\n",
       "        -7.0098e-01, -2.0204e-01, -1.6607e+00,  1.4713e+00,  4.1149e-01,\n",
       "        -1.6247e-02,  5.8324e-01, -1.1598e+00, -4.6569e-01,  1.4136e-01,\n",
       "        -3.8785e-01, -5.4999e-01,  8.0080e-01, -1.8471e-01,  3.9579e-01,\n",
       "        -6.6215e-01, -2.1097e+00,  1.3813e+00,  1.3005e+00, -3.4980e-01,\n",
       "         2.9603e-02, -3.7135e-01, -5.0055e-01, -5.5823e-01,  1.5811e-01],\n",
       "       grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write your code\n",
    "out_mean = out.mean(dim=0)\n",
    "out_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have understood the first steps of SBERT, we can implement them in a class.\n",
    "\n",
    "In the next cell, write the `forward()` method that takes the two sentences as input in the form of two `LongTensor` of indices.\n",
    "1. Extract their embeddings from the GloVe embedding matrix.\n",
    "2. Encode them with the transformer, and \n",
    "3. Compute their respective mean. You will call these vectors $\\mathbf{u}$ and $\\mathbf{v}$\n",
    "4. Return these two vectors of means. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the forward method\n",
    "class SBERT(nn.Module):\n",
    "    def __init__(self, nbr_classes, glove, d_model=50, nhead=5, num_layers=4):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embeddings = nn.Embedding(\n",
    "            glove.size()[0],\n",
    "            glove.size()[1],\n",
    "            padding_idx=0).from_pretrained(glove)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model, nhead)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            self.encoder_layer, num_layers=num_layers)\n",
    "        # We do not use this last line for now\n",
    "        self.fc = nn.Linear(3 * d_model, nbr_classes)\n",
    "\n",
    "    def forward(self, u, v):\n",
    "        u = self.embeddings(u)\n",
    "        v = self.embeddings(v)\n",
    "        u = self.transformer_encoder(u)\n",
    "        v = self.transformer_encoder(v)\n",
    "        u = u.mean(dim=0)\n",
    "        v = v.mean(dim=0)\n",
    "        return u, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert = SBERT(3, glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SBERT(\n",
       "  (embeddings): Embedding(400003, 50)\n",
       "  (encoder_layer): TransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=50, out_features=50, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=50, out_features=2048, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=2048, out_features=50, bias=True)\n",
       "    (norm1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=50, out_features=50, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=50, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=50, bias=True)\n",
       "        (norm1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=150, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbert.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([   10,   902,    16,    10,  2870, 11073,    77,    10,  2324,   138,\n",
       "          7353,     5]),\n",
       " tensor([  10,  902,   17,  791,   29, 2870,   13,   10,  994,    5]),\n",
       " tensor(2))"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, v = sbert(dataset[0][0], dataset[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.6700, -0.2360,  1.2674,  1.6374,  0.6937,  0.3883, -0.1752,  0.6734,\n",
       "          0.0650,  0.9745, -1.0740, -0.7048, -0.5824,  1.0721,  0.6225,  0.9510,\n",
       "         -0.9779,  0.5997, -0.6038, -1.3028,  0.1752,  1.3470,  0.3807, -0.3907,\n",
       "          1.3703, -1.7921, -1.7282,  0.6793, -0.2297,  0.0057,  0.8187, -0.6626,\n",
       "         -0.2242,  0.2350,  1.8458,  0.5767, -0.0040, -0.2891,  0.3414, -1.3352,\n",
       "         -0.2594, -0.8940, -0.1071,  1.1776, -0.3841, -0.7711, -0.3386, -0.9680,\n",
       "          0.9540, -2.1477], grad_fn=<MeanBackward1>),\n",
       " tensor([-0.6072, -0.0978,  0.8420,  1.6481,  0.3514,  0.2331, -0.1193,  0.8497,\n",
       "          0.0343,  1.1335, -0.6015, -0.0607, -0.7210,  0.5908,  0.6622,  1.2558,\n",
       "         -0.6479,  0.3086, -0.7852, -1.0935,  0.3379,  1.1170,  0.4885, -0.7003,\n",
       "          1.6507, -1.4778, -1.9510,  0.5244, -0.1333, -1.0121,  1.0102, -0.5893,\n",
       "         -0.1246, -0.2205,  2.0341,  0.7589, -0.2853, -0.3439,  0.5471, -1.3470,\n",
       "          0.1572, -1.3095, -0.1394,  1.0948, -0.4739, -0.5386, -0.8167, -0.4670,\n",
       "          1.2539, -2.2201], grad_fn=<MeanBackward1>))"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u, v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the code that concatenate `u`, `v`, and `|u-v|`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.6700, -0.2360,  1.2674,  1.6374,  0.6937,  0.3883, -0.1752,  0.6734,\n",
       "         0.0650,  0.9745, -1.0740, -0.7048, -0.5824,  1.0721,  0.6225,  0.9510,\n",
       "        -0.9779,  0.5997, -0.6038, -1.3028,  0.1752,  1.3470,  0.3807, -0.3907,\n",
       "         1.3703, -1.7921, -1.7282,  0.6793, -0.2297,  0.0057,  0.8187, -0.6626,\n",
       "        -0.2242,  0.2350,  1.8458,  0.5767, -0.0040, -0.2891,  0.3414, -1.3352,\n",
       "        -0.2594, -0.8940, -0.1071,  1.1776, -0.3841, -0.7711, -0.3386, -0.9680,\n",
       "         0.9540, -2.1477, -0.6072, -0.0978,  0.8420,  1.6481,  0.3514,  0.2331,\n",
       "        -0.1193,  0.8497,  0.0343,  1.1335, -0.6015, -0.0607, -0.7210,  0.5908,\n",
       "         0.6622,  1.2558, -0.6479,  0.3086, -0.7852, -1.0935,  0.3379,  1.1170,\n",
       "         0.4885, -0.7003,  1.6507, -1.4778, -1.9510,  0.5244, -0.1333, -1.0121,\n",
       "         1.0102, -0.5893, -0.1246, -0.2205,  2.0341,  0.7589, -0.2853, -0.3439,\n",
       "         0.5471, -1.3470,  0.1572, -1.3095, -0.1394,  1.0948, -0.4739, -0.5386,\n",
       "        -0.8167, -0.4670,  1.2539, -2.2201,  0.0628,  0.1382,  0.4255,  0.0107,\n",
       "         0.3423,  0.1552,  0.0559,  0.1764,  0.0307,  0.1590,  0.4724,  0.6441,\n",
       "         0.1386,  0.4813,  0.0397,  0.3048,  0.3300,  0.2911,  0.1814,  0.2094,\n",
       "         0.1627,  0.2300,  0.1078,  0.3096,  0.2804,  0.3143,  0.2227,  0.1549,\n",
       "         0.0964,  1.0178,  0.1914,  0.0733,  0.0996,  0.4555,  0.1883,  0.1822,\n",
       "         0.2813,  0.0548,  0.2057,  0.0118,  0.4166,  0.4155,  0.0323,  0.0828,\n",
       "         0.0898,  0.2325,  0.4781,  0.5010,  0.2999,  0.0723],\n",
       "       grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write your code\n",
    "diff = u - v\n",
    "diff = abs(diff)\n",
    "concat_tensor = torch.cat((u, v, diff), dim=0)\n",
    "concat_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to add the logistic regression head. Complement the `forward()` method so that it concatenates, $\\mathbf{u}$, $\\mathbf{v}$, and $\\mathbf{|u - v|}$ and outputs three classes. You have only two lines to add."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the forward() method\n",
    "class SBERT(nn.Module):\n",
    "    def __init__(self, nbr_classes, glove, d_model=50, nhead=5, num_layers=4):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embeddings = nn.Embedding(glove.size()[0],\n",
    "                                       glove.size()[1],\n",
    "                                       padding_idx=0).from_pretrained(glove)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model, nhead)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            self.encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(3 * d_model, nbr_classes)\n",
    "\n",
    "    def forward(self, u, v):\n",
    "        u = self.embeddings(u)\n",
    "        v = self.embeddings(v)\n",
    "        u = self.transformer_encoder(u)\n",
    "        v = self.transformer_encoder(v)\n",
    "        u = u.mean(dim=0)\n",
    "        v = v.mean(dim=0)\n",
    "        diff = abs(u - v)\n",
    "        x = torch.cat((u, v, diff), dim=0)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert = SBERT(3, glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SBERT(\n",
       "  (embeddings): Embedding(400003, 50)\n",
       "  (encoder_layer): TransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=50, out_features=50, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=50, out_features=2048, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=2048, out_features=50, bias=True)\n",
       "    (norm1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=50, out_features=50, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=50, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=50, bias=True)\n",
       "        (norm1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=150, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbert.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([   10,   902,    16,    10,  2870, 11073,    77,    10,  2324,   138,\n",
       "          7353,     5]),\n",
       " tensor([  10,  902,   17,  791,   29, 2870,   13,   10,  994,    5]),\n",
       " tensor(2))"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the complete model and we have three outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0482, -0.7644, -0.3482], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbert(dataset[0][0], dataset[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training SBERT\n",
    "We now train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SBERT(\n",
       "  (embeddings): Embedding(400003, 50)\n",
       "  (encoder_layer): TransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=50, out_features=50, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=50, out_features=2048, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=2048, out_features=50, bias=True)\n",
       "    (norm1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=50, out_features=50, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=50, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=50, bias=True)\n",
       "        (norm1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=150, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbert.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()    # cross entropy loss\n",
    "optimizer = torch.optim.Adam(sbert.parameters(), lr=0.00002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([   10,   902,    16,    10,  2870, 11073,    77,    10,  2324,   138,\n",
       "          7353,     5]),\n",
       " tensor([  10,  902,   17,  791,   29, 2870,   13,   10,  994,    5]),\n",
       " tensor(2))"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the training loop. You will process one sample at a time to simplify it i.e. no batch. Record the training loss.\n",
    "\n",
    "A better design would use a `Dataset` object. We will see this construct in the last laboratory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 5493/5493 [01:46<00:00, 51.41it/s]\n",
      "100%|███████████████████████████████████████| 5493/5493 [01:46<00:00, 51.45it/s]\n",
      "100%|███████████████████████████████████████| 5493/5493 [01:45<00:00, 51.99it/s]\n",
      "100%|███████████████████████████████████████| 5493/5493 [01:42<00:00, 53.43it/s]\n",
      "100%|███████████████████████████████████████| 5493/5493 [01:39<00:00, 55.37it/s]\n"
     ]
    }
   ],
   "source": [
    "# Write your code\n",
    "ce_train_loss = []\n",
    "for epoch in range(5):\n",
    "    loss_train = 0\n",
    "    for data in tqdm(dataset, ncols=80):\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = sbert(data[0], data[1])\n",
    "        loss = loss_fn(y_pred.view(1, -1), data[2].view(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_train += loss.item()\n",
    "    ce_train_loss.append(loss_train / len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(range(len(ce_train_loss)),\n",
    "            ce_train_loss, c='b', marker='x')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Cross entropy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(sbert, 'sbert_mini.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_saved = torch.load('sbert_mini.pt', weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  10,  902,   17,  791,   29, 2870,   13,   10,  994,    5])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.0267, -1.4220,  0.5307], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbert(dataset[0][0], dataset[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write a function to apply the model to a sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sbert(model, token_indices):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        v = model.embeddings(token_indices)\n",
    "        v = model.transformer_encoder(v).mean(dim=0)\n",
    "        return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.3360,  0.3409, -0.9380, -0.8124, -1.0140, -0.7726, -1.2169, -0.4032,\n",
       "         0.3025, -0.3208, -0.4376, -1.6434,  0.3553,  1.0619,  0.3100, -1.6934,\n",
       "         0.3593,  0.5016, -0.5657, -1.3219,  0.5176,  0.1548, -0.5861,  0.2963,\n",
       "         0.3884, -0.0198, -0.5850,  0.6764, -0.2426,  0.2178,  1.0068, -0.2134,\n",
       "        -0.5583, -0.9864,  0.3220,  0.1011,  0.0829,  0.2579, -0.5938,  1.1489,\n",
       "         0.0834,  2.5196, -0.0169,  1.7132,  0.1598,  0.1371,  0.3230,  0.5830,\n",
       "        -1.0171,  0.6784])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_sbert(sbert, dataset[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'entailment', 1: 'contradiction', 2: 'neutral'}"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the model to all our pairs and, for each pair, we compute the cosine of the resulting embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 5493/5493 [00:21<00:00, 256.06it/s]\n"
     ]
    }
   ],
   "source": [
    "cos_sim = {'entailment': 0.0, 'neutral': 0.0, 'contradiction': 0.0}\n",
    "cnt = {'entailment': 0.0, 'neutral': 0.0, 'contradiction': 0.0}\n",
    "sbert.eval()\n",
    "\n",
    "for data in tqdm(dataset, ncols=80):\n",
    "    cos_val = compute_cosine(\n",
    "        encode_sbert(sbert, data[0]),\n",
    "        encode_sbert(sbert, data[1]))\n",
    "    cos_sim[idx2label[data[2].item()]] += cos_val\n",
    "    cnt[idx2label[data[2].item()]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'entailment': tensor(1282.3234),\n",
       "  'neutral': tensor(929.5768),\n",
       "  'contradiction': tensor(730.1921)},\n",
       " {'entailment': 1839.0, 'neutral': 1821.0, 'contradiction': 1833.0})"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim, cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entailment': tensor(0.6973),\n",
       " 'neutral': tensor(0.5105),\n",
       " 'contradiction': tensor(0.3984)}"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for key in cos_sim.keys():\n",
    "    cos_sim[key] /= cnt[key]\n",
    "cos_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying the Embedder to Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"The weather is lovely today.\",\n",
    "    \"It's so sunny outside!\",\n",
    "    \"He drove to the stadium.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the', 'weather', 'is', 'lovely', 'today', '.'],\n",
       " ['it', \"'\", 's', 'so', 'sunny', 'outside', '!'],\n",
       " ['he', 'drove', 'to', 'the', 'stadium', '.']]"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sents = [tokenize(sent, pattern) for sent in sentences]\n",
    "tokenized_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([    3,  1623,    17, 11130,   376,     5]),\n",
       " tensor([  23,   60, 1537,  103, 9328,  590,  808]),\n",
       " tensor([  21, 3189,    7,    3, 1355,    5])]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexed_sents = [convert_symbols(sent, token2idx) for sent in tokenized_sents]\n",
    "indexed_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([ 2.2454e+00,  3.7940e-02, -5.0931e-01, -2.0021e+00, -1.1024e-03,\n",
       "         -1.0748e+00, -1.8613e+00, -4.2466e-01,  2.3933e-01,  5.7533e-01,\n",
       "         -5.0346e-01, -1.1148e+00,  6.5425e-01, -3.5272e-01,  1.3769e-02,\n",
       "         -4.3552e-01,  1.0186e+00,  4.6030e-01,  5.0811e-01, -8.6473e-01,\n",
       "          5.4439e-01,  1.8124e-02,  1.4562e-01,  6.4463e-01,  3.8474e-03,\n",
       "          7.6681e-01, -6.6348e-01,  7.8563e-02, -2.3934e-01, -4.6105e-01,\n",
       "          1.7292e+00,  8.2733e-01, -1.4361e-01, -3.8671e-01, -1.8815e-01,\n",
       "          1.2516e-01,  4.1157e-05,  2.8138e-01, -2.0957e+00,  6.8989e-01,\n",
       "          7.5227e-01,  2.4687e+00,  3.4086e-01, -1.9346e-01,  2.8161e-01,\n",
       "         -2.8446e-01,  5.4388e-01, -1.2398e-01, -1.3780e+00, -7.3617e-01]),\n",
       " tensor([ 1.8934,  0.8095,  0.6403, -0.9870,  0.0032, -0.6377, -0.2264, -0.6243,\n",
       "         -0.4743,  1.2878, -0.1741, -0.3069,  0.9260, -1.6074, -0.7531, -0.4762,\n",
       "          0.4599,  1.1698,  1.6963, -0.6498,  0.8393,  0.1599,  0.0533, -0.2539,\n",
       "          0.0335,  0.7462, -0.4895, -0.3008, -0.1460,  0.0163,  1.2891,  0.4540,\n",
       "          0.2025,  0.1341,  0.2347, -0.0318, -0.1825, -0.3379, -1.7806, -0.0400,\n",
       "         -0.0172,  0.7663, -0.6548, -0.6587,  0.5576, -0.9155,  0.6879, -0.4297,\n",
       "         -1.4494, -0.4669]),\n",
       " tensor([ 0.9946, -0.4313,  0.0699,  0.5662, -0.2426, -1.4126,  0.2404,  0.3259,\n",
       "         -1.5159,  0.4449,  0.3088, -0.0188,  0.7133, -0.6691, -1.6863, -0.5636,\n",
       "          0.3238,  1.0679,  2.1881,  0.0549,  0.6002,  0.8789, -0.1681, -0.4044,\n",
       "         -0.0451, -0.0723,  0.3460, -0.1308,  0.2667,  0.3305,  0.0042,  1.1649,\n",
       "          0.2224, -1.7401,  1.0075,  1.2474, -0.2880,  0.0861, -0.5421,  0.8384,\n",
       "         -0.1925, -0.7065, -0.1979, -0.2447,  0.8265, -0.1604, -0.1364, -0.0533,\n",
       "         -2.1449, -1.3457])]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_sents = [encode_sbert(sbert, sent) for sent in indexed_sents]\n",
    "encoded_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.7064, 0.2633],\n",
       "        [0.7064, 1.0000, 0.5401],\n",
       "        [0.2633, 0.5401, 1.0000]])"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([compute_cosine(s1, s2)\n",
    "for s1 in encoded_sents\n",
    "for s2 in encoded_sents]).reshape(len(sentences), len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turning in your assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now your are done with the program. To complete this assignment, you will write a report where you will:\n",
    "1. Write a short individual report on your program. I recommend that you use this structure for your report:\n",
    "      1. Objectives and dataset\n",
    "      2. Method and program structure, where you should outline your program and possibly describe difficult parts.\n",
    "      3. Results.\n",
    "      4. Conclusion.\n",
    "2. In Sect. _Method and program structure_, do not forget to:\n",
    "   * Summarize the baseline\n",
    "   * Summarize SBERT\n",
    "\n",
    "The whole report should be of 2 to 3 pages.\n",
    "\n",
    "Submit your report as well as your **notebook** (for archiving purposes) to Canvas: https://canvas.education.lu.se/. To write your report, use Latex. This will probably help you structure your text. You can use the Overleaf online editor (www.overleaf.com). You will then upload a PDF file in Canvas.\n",
    "\n",
    "The submission deadline is October 18, 2024."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
